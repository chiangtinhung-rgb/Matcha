{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPveRoU/ZWg2JB7npv7fv7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiangtinhung-rgb/Matcha/blob/main/HW4_%E6%96%87%E5%AD%97%E8%B3%87%E6%96%99%E5%B0%8F%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW4-æ–‡å­—è³‡æ–™å°åˆ†æ\n",
        "ç›®æ¨™ï¼šå¾ Sheet è®€é–‹æ”¾å¼å›ç­” â†’ åšè©æ•¸èˆ‡é—œéµå­—è¨ˆæ•¸ â†’ è¼¸å‡ºå‰ N ç†±è© â†’ å›å¯«çµ±è¨ˆè¡¨ã€‚\n",
        "\n",
        "AI é»å­ï¼šè«‹æ¨¡å‹ç”¢å‡º 5 å¥æ´å¯Ÿæ‘˜è¦ + ä¸€æ®µ 120 å­—çµè«–ã€‚\n"
      ],
      "metadata": {
        "id": "2SdUJNtVXA7G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvZ_3VmWWmg8"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "               gradio pandas beautifulsoup4 google-generativeai python-dateutil"
      ],
      "metadata": {
        "id": "IxdXRAuYMure"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ–°å¢ TF-IDF ç›¸é—œå¥—ä»¶\n",
        "import jieba\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "o9araqEtWwdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, uuid, re, json, datetime\n",
        "from datetime import datetime as dt, timedelta\n",
        "from dateutil.tz import gettz\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Google Auth & Sheets\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2 import service_account\n",
        "from google.auth import default"
      ],
      "metadata": {
        "id": "BzthwvSkTDVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "ASYrK7EYTIPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# å¾ Colab Secrets ä¸­ç²å– API é‡‘é‘°\n",
        "api_key = userdata.get('MATCHA')\n",
        "\n",
        "# ä½¿ç”¨ç²å–çš„é‡‘é‘°é…ç½® genai\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.5-pro')"
      ],
      "metadata": {
        "id": "MhbbqXUVTJSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHEET_URL = \"https://docs.google.com/spreadsheets/d/1qR5tsLMzL1tVHRQvkg3gHrznzEWlVGIunhS-I7m9rzU/edit?usp=sharing\"\n",
        "WORKSHEET_NAME = \"å·¥ä½œè¡¨1\"\n",
        "TIMEZONE = \"Asia/Taipe\""
      ],
      "metadata": {
        "id": "zNib2vWhTPRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PTT_HEADER = [\n",
        "    \"post_id\",\"title\",\"url\",\"date\",\"author\",\"nrec\",\"created_at\",\n",
        "    \"fetched_at\",\"content\"\n",
        "]\n",
        "TERMS_HEADER = [\"term\",\"freq\",\"df_count\",\"tfidf_mean\",\"examples\"]"
      ],
      "metadata": {
        "id": "Cz3whVbaU_WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_spreadsheet(name):\n",
        "    try:\n",
        "        sh = gc.open(name)  # returns gspread.models.Spreadsheet\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        sh = gc.create(name)\n",
        "    return sh\n",
        "\n",
        "sh = ensure_spreadsheet(WORKSHEET_NAME)"
      ],
      "metadata": {
        "id": "sHGSAmywUS9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_worksheet(sh, title, header):\n",
        "    try:\n",
        "        ws = sh.worksheet(title)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=title, rows=\"1000\", cols=str(len(header)+5))\n",
        "        ws.update([header])\n",
        "    # è‹¥æ²’æœ‰è¡¨é ­å°±è£œä¸Š\n",
        "    data = ws.get_all_values()\n",
        "    if not data or (data and data[0] != header):\n",
        "        ws.clear()\n",
        "        ws.update([header])\n",
        "    return ws"
      ],
      "metadata": {
        "id": "l-9sJPlLUYP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ws_ptt_posts = ensure_worksheet(sh, \"ptt_movie_posts\", PTT_HEADER)\n",
        "ws_ptt_terms = ensure_worksheet(sh, \"ptt_movie_terms\", TERMS_HEADER)"
      ],
      "metadata": {
        "id": "lz-_uk8kUZVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_data = []"
      ],
      "metadata": {
        "id": "XYgnQHfiXrNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}"
      ],
      "metadata": {
        "id": "hoSHiN0VXuJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_previous_page_url(soup):\n",
        "    \"\"\"ç²å–ä¸Šä¸€é  (æ›´æ–°ã€æ›´èˆŠçš„æ–‡ç« ) çš„é€£çµ\"\"\"\n",
        "    # PTT çš„ã€Œä¸Šä¸€é ã€æŒ‰éˆ• HTML çµæ§‹ï¼šdiv class=\"btn-group btn-group-paging\"\n",
        "    paging_div = soup.find('div', class_='btn-group btn-group-paging')\n",
        "    if paging_div:\n",
        "        # PTT é é¢æŒ‰éˆ•é †åºï¼šæœ€èˆŠ (0), ä¸Šé  (1), ä¸‹é  (2), æœ€æ–° (3)\n",
        "        # æˆ‘å€‘è¦æ‰¾çš„æ˜¯ã€Œä¸Šä¸€é ã€ (å³æ›´èˆŠçš„æ–‡ç« ) çš„é€£çµï¼Œç´¢å¼•ç‚º 1\n",
        "        prev_button = paging_div.find_all('a')[1]\n",
        "\n",
        "        # æª¢æŸ¥é€£çµæ˜¯å¦æœ‰æ•ˆ (å¦‚æœå·²ç¶“æ˜¯ç¬¬ä¸€é ï¼Œé€£çµæœƒæ˜¯ '#' æˆ–æ²’æœ‰ href)\n",
        "        if 'href' in prev_button.attrs:\n",
        "            return \"https://www.ptt.cc\" + prev_button['href']\n",
        "    return None"
      ],
      "metadata": {
        "id": "SJdcDWlgtzED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responseIndex = requests.get(\"https://www.ptt.cc/bbs/movie/index.html\", headers=headers, timeout=5)\n",
        "html_contentIndex = responseIndex.text\n",
        "soupIndex = BeautifulSoup(html_contentIndex, 'html.parser')"
      ],
      "metadata": {
        "id": "gsVSz5F2P1Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_url = get_previous_page_url(soupIndex)"
      ],
      "metadata": {
        "id": "QwW99_jwP75D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_url"
      ],
      "metadata": {
        "id": "x1K4uTfPQVzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_index_split(url):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨å­—ä¸²åˆ†å‰²æ–¹æ³•è§£æé ç¢¼ã€‚\n",
        "\n",
        "    é‚è¼¯ï¼š\n",
        "    1. ä»¥ 'index' åˆ†å‰²ç¶²å€ï¼š['...', '10808.html']\n",
        "    2. å–ç¬¬äºŒå€‹å…ƒç´  ('10808.html')\n",
        "    3. ä»¥ '.html' åˆ†å‰²ï¼š['10808', '']\n",
        "    4. å–ç¬¬ä¸€å€‹å…ƒç´  ('10808')\n",
        "    5. è½‰æ›ç‚ºæ•´æ•¸\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # ç¢ºä¿ç¶²å€ä¸­åŒ…å« 'index' å’Œ '.html'\n",
        "        index_str = url.split('index')[1].split('.html')[0]\n",
        "        return int(index_str)\n",
        "    except IndexError:\n",
        "        print(\"éŒ¯èª¤: ç¶²å€çµæ§‹ä¸ç¬¦åˆé æœŸ (ç¼ºå°‘ 'index' æˆ– '.html')\")\n",
        "        return None\n",
        "    except ValueError:\n",
        "        print(\"éŒ¯èª¤: æå–åˆ°çš„å…§å®¹ç„¡æ³•è½‰æ›ç‚ºæ•¸å­—\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "a3uyvNdzQxoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æŒ‡å®šçš„èµ·å§‹é ç¢¼\n",
        "START_INDEX = extract_index_split(current_url)\n",
        "# æƒ³è¦å–å¾—çš„é æ•¸\n",
        "PAGES_TO_FETCH = 10\n",
        "\n",
        "# PTT åŸºç¤ç¶²å€\n",
        "BASE_URL = \"https://www.ptt.cc/bbs/movie/index\"\n",
        "\n",
        "# è¨ˆç®—è¿´åœˆçš„çµæŸé» (ä¾‹å¦‚: 10808 - 10 + 1 = 10799)\n",
        "# range(start, stop, step) çš„ stop æ˜¯ä¸åŒ…å«çš„ï¼Œæ‰€ä»¥æˆ‘å€‘è¨­ç‚º START_INDEX - PAGES_TO_FETCH\n",
        "stop_index = START_INDEX - PAGES_TO_FETCH\n",
        "\n",
        "print(f\"--- æ­£åœ¨ç”Ÿæˆå¾ {START_INDEX} åˆ° {stop_index + 1} çš„ {PAGES_TO_FETCH} å€‹ç¶²å€ ---\")\n",
        "\n",
        "# ä½¿ç”¨ range è¿´åœˆï¼Œå¾ START_INDEX éæ¸›åˆ° stop_index\n",
        "for index in range(START_INDEX, stop_index, -1):\n",
        "    # çµ„åˆå®Œæ•´çš„ URL\n",
        "    url = f\"{BASE_URL}{index}.html\"\n",
        "    print(url)\n",
        "    response = requests.get(url, headers=headers, timeout=5)\n",
        "    html_content = response.text\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    article_list = soup.find_all('div', class_='r-ent')\n",
        "    for article in article_list:\n",
        "        # æ¨™é¡Œ (Title) å’Œ é€£çµ (Href) è³‡è¨Šé€šå¸¸åœ¨ div class=\"title\" å…§\n",
        "        title_tag = article.find('div', class_='title').find('a')\n",
        "\n",
        "        # æ’é™¤è¢«åˆªé™¤æˆ–ä¸å¯å­˜å–(æ¨™é¡Œç‚º - )çš„æ–‡ç« \n",
        "        if title_tag:\n",
        "            title = title_tag.text.strip()\n",
        "            href = \"https://www.ptt.cc\" + title_tag['href']\n",
        "        else:\n",
        "            # è™•ç†è¢«åˆªé™¤çš„æ–‡ç«  (é€šå¸¸æ¨™é¡Œæœƒæ˜¯ '-')\n",
        "            title = article.find('div', class_='title').text.strip()\n",
        "            href = \"N/A (å·²åˆªé™¤æˆ–ä¸å¯å­˜å–)\"\n",
        "\n",
        "        # ä½œè€… (Author) è³‡è¨Šåœ¨ div class=\"author\" å…§\n",
        "        author = article.find('div', class_='author').text.strip()\n",
        "\n",
        "        # æ—¥æœŸ (Date) è³‡è¨Šåœ¨ div class=\"date\" å…§\n",
        "        date = article.find('div', class_='date').text.strip()\n",
        "\n",
        "        # å„²å­˜è³‡æ–™\n",
        "        articles_data.append({\n",
        "            'title': title,\n",
        "            'date': date,\n",
        "            'author': author,\n",
        "            'href': href\n",
        "        })\n",
        "\n",
        "        # é¡å¤–è¦æ±‚ï¼šå¦‚æœæ¨™é¡Œæ˜¯æŒ‡å®šæ¨™é¡Œï¼Œå°å‡ºå…¶å°æ‡‰çš„ href\n",
        "        # å‡è¨­æˆ‘å€‘æŒ‡å®šè¦ç‰¹åˆ¥é—œæ³¨æ¨™é¡ŒåŒ…å« \"æ–°è\" çš„æ–‡ç« \n",
        "        # if 'æ–°è' in title:\n",
        "        #     print(f\"[ç‰¹åˆ¥é—œæ³¨] æ¨™é¡Œï¼š{title} | é€£çµï¼š{href}\")\n",
        "    # è¨»é‡‹: åœ¨å¯¦éš›çš„çˆ¬èŸ²ç¨‹å¼ä¸­ï¼Œæ‚¨æœƒå°‡é€™å€‹ url å‚³éçµ¦ requests.get() å‡½å¼ä¾†ç²å–å…§å®¹\n",
        "    # ä¾‹å¦‚: html_content = requests.get(url, headers=headers, cookies=cookies).text"
      ],
      "metadata": {
        "id": "W9wtW_iWqTCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_data"
      ],
      "metadata": {
        "id": "IBGIaxV7rT5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = articles_data[1]['title']"
      ],
      "metadata": {
        "id": "1vfOZfJqZYTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "\n",
        "# å¾…åˆ†è©çš„ä¸­æ–‡å¥å­\n",
        "print(f\"åŸå§‹å¥å­: {sentence}\\n\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# --- 1. ç²¾ç¢ºæ¨¡å¼ (Default Mode) ---\n",
        "# é€™æ˜¯æœ€å¸¸ç”¨çš„æ¨¡å¼ï¼Œå˜—è©¦å°‡å¥å­æœ€ç²¾ç¢ºåœ°åˆ‡é–‹ï¼Œé©åˆæ–‡æœ¬åˆ†æã€‚\n",
        "print(\"æ¨¡å¼ä¸€ï¼šç²¾ç¢ºæ¨¡å¼ (jieba.cut)\")\n",
        "# jieba.cut è¿”å›çš„æ˜¯ä¸€å€‹è¿­ä»£å™¨ (iterator)\n",
        "seg_list_precise = jieba.cut(sentence, cut_all=False)\n",
        "# ä½¿ç”¨ '/ ' å°‡çµæœä¸²æ¥èµ·ä¾†ï¼Œæ–¹ä¾¿è¼¸å‡º\n",
        "print(f\"åˆ†è©çµæœ: {'/ '.join(seg_list_precise)}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# --- 2. å…¨æ¨¡å¼ (Full Mode) ---\n",
        "# æœƒæƒæå‡ºå¥å­ä¸­æ‰€æœ‰å¯èƒ½çš„è©èªï¼Œé€Ÿåº¦æœ€å¿«ï¼Œä½†çµæœå¯èƒ½æœ‰å¤§é‡é‡ç–Šã€‚\n",
        "print(\"æ¨¡å¼äºŒï¼šå…¨æ¨¡å¼ (jieba.cut(..., cut_all=True))\")\n",
        "seg_list_all = jieba.cut(sentence, cut_all=True)\n",
        "print(f\"åˆ†è©çµæœ: {'/ '.join(seg_list_all)}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# --- 3. æœå°‹å¼•æ“æ¨¡å¼ (Search Engine Mode) ---\n",
        "# åœ¨ç²¾ç¢ºæ¨¡å¼çš„åŸºç¤ä¸Šï¼Œå°é•·è©å†æ¬¡é€²è¡Œç´°åˆ†ï¼Œé©åˆç”¨æ–¼æœå°‹å¼•æ“å»ºç«‹ç´¢å¼•ã€‚\n",
        "print(\"æ¨¡å¼ä¸‰ï¼šæœå°‹å¼•æ“æ¨¡å¼ (jieba.cut_for_search)\")\n",
        "seg_list_search = jieba.cut_for_search(sentence)\n",
        "print(f\"åˆ†è©çµæœ: {'/ '.join(seg_list_search)}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# --- 4. è©æ€§æ¨™è¨» (Optional: Add Part-of-Speech Tagging) ---\n",
        "# jieba.posseg å¯ä»¥åœ¨åˆ†è©çš„åŒæ™‚æ¨™è¨»è©æ€§ (ä¾‹å¦‚ n: åè©, v: å‹•è©)\n",
        "import jieba.posseg as pseg\n",
        "print(\"æ¨¡å¼å››ï¼šåˆ†è©èˆ‡è©æ€§æ¨™è¨» (jieba.posseg)\")\n",
        "words = pseg.cut(sentence)\n",
        "result = []\n",
        "for word, flag in words:\n",
        "    result.append(f\"{word}/{flag}\")\n",
        "\n",
        "print(f\"åˆ†è©çµæœ: {' '.join(result)}\")\n",
        "print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "8Lip1mACZbzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 1. åˆå§‹åŒ–ä¸€å€‹ Counter ç‰©ä»¶ä¾†è¨˜éŒ„æ‰€æœ‰å–®è©çš„é »ç‡\n",
        "word_counts = Counter()\n",
        "\n",
        "# 2. è¨­å®šä¸€å€‹ç¯„åœï¼Œå¾ç´¢å¼• 1 åˆ° 10 (åŒ…å«)\n",
        "# åœ¨ Python çš„ range ä¸­ï¼Œrange(start, stop) æ˜¯å¾ start åˆ° stop-1\n",
        "for i in range(1, 11):\n",
        "    # ç¢ºä¿ç´¢å¼• i å­˜åœ¨ï¼Œé¿å…éŒ¯èª¤\n",
        "    if i < len(articles_data) and 'title' in articles_data[i]:\n",
        "        # å–å‡ºæ¨™é¡Œå…§å®¹\n",
        "        title_text = articles_data[i]['title']\n",
        "\n",
        "        # --- æ–‡æœ¬æ¸…ç†æ­¥é©Ÿ (é‡è¦) ---\n",
        "        # æ¸…é™¤æ¨™é»ç¬¦è™Ÿã€ç©ºæ ¼ã€æ›è¡Œç¬¦ç­‰éä¸­æ–‡å­—ç¬¦\n",
        "        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å­—æ¯ã€æ•¸å­—\n",
        "        cleaned_text = re.sub(r'[^\\w\\s]', '', title_text) # ç§»é™¤å¤§éƒ¨åˆ†æ¨™é»ç¬¦è™Ÿ\n",
        "\n",
        "        # é€²è¡Œçµå·´åˆ†è© (ä½¿ç”¨ç²¾ç¢ºæ¨¡å¼ jieba.cut)\n",
        "        # cut() è¿”å›çš„æ˜¯ä¸€å€‹ generatorï¼Œé€šå¸¸æœƒè½‰æˆ list\n",
        "        # æˆ–ç›´æ¥åœ¨è¿´åœˆä¸­ä½¿ç”¨ï¼Œé€™è£¡æˆ‘å€‘ç›´æ¥ç”¨ä¾†æ›´æ–° Counter\n",
        "        words = jieba.cut(cleaned_text, cut_all=False)\n",
        "\n",
        "        # å°‡åˆ†è©çµæœæ›´æ–°åˆ° word_counts\n",
        "        # Counter çš„ update() æ–¹æ³•å¯ä»¥ç›´æ¥æ¥æ”¶ä¸€å€‹å¯ç–Šä»£å°è±¡ï¼ˆå¦‚ wordsï¼‰\n",
        "        word_counts.update(words)\n",
        "\n",
        "# 3. æ¸…ç†åˆ†è©çµæœï¼Œç§»é™¤ç©ºæ ¼ã€å–®å€‹å­—æ¯ç­‰å¸¸è¦‹é›œè¨Š\n",
        "# å»ºç«‹ä¸€å€‹æ–°çš„ Counterï¼ŒåªåŒ…å«é•·åº¦å¤§æ–¼ 1 çš„è©ï¼Œæˆ–æ‚¨èªç‚ºæœ‰æ„ç¾©çš„è©\n",
        "final_word_counts = Counter()\n",
        "for word, count in word_counts.items():\n",
        "    # ç§»é™¤ç©ºå­—ç¬¦ä¸²ã€ç©ºæ ¼ã€æ›è¡Œç¬¦\n",
        "    if word.strip() and len(word.strip()) > 1:\n",
        "        final_word_counts[word] = count\n",
        "\n",
        "# 4. è¼¸å‡ºè©é »çµæœ (ä¾‹å¦‚ï¼Œå‰ 10 å€‹é«˜é »è©)\n",
        "print(\"--- è©é »çµ±è¨ˆçµæœ (å‰ 10 å) ---\")\n",
        "for word, count in final_word_counts.most_common(10):\n",
        "    print(f\"'{word}': {count} æ¬¡\")"
      ],
      "metadata": {
        "id": "MbvxTVkPZe1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "\n",
        "# è¨­ç½®ä¸€å€‹ç°¡å–®çš„åœç”¨è©åˆ—è¡¨ï¼ˆStop Wordsï¼‰\n",
        "# é€™äº›è©é€šå¸¸é »ç‡å¾ˆé«˜ï¼Œä½†å°æ–‡ç« ä¸»é¡Œè²¢ç»å°\n",
        "stopwords = set(['çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'ä¹‹', 'ä¸€å€‹', 'å’Œ', 'è¨è«–', 'åˆ†äº«'])"
      ],
      "metadata": {
        "id": "ota8nHkeZodB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_list = []\n",
        "\n",
        "# å¾ç´¢å¼• 1 åˆ° 10 (åŒ…å«)\n",
        "for i in range(1, 11):\n",
        "    if i < len(articles_data) and 'title' in articles_data[i]:\n",
        "        title_text = articles_data[i]['title']\n",
        "\n",
        "        # æ–‡æœ¬æ¸…ç†ï¼šç§»é™¤æ¨™é»ç¬¦è™Ÿå’Œéè©èªå­—ç¬¦\n",
        "        cleaned_text = re.sub(r'[^\\w\\s]', '', title_text)\n",
        "\n",
        "        # é€²è¡Œçµå·´åˆ†è©\n",
        "        # lcut() ç›´æ¥è¿”å›ä¸€å€‹åˆ—è¡¨\n",
        "        words = jieba.lcut(cleaned_text, cut_all=False)\n",
        "\n",
        "        # éæ¿¾åœç”¨è©å’Œå–®å€‹ç©ºç™½è©ï¼Œä¸¦ç”¨ç©ºæ ¼é‡æ–°é€£æ¥æˆä¸€å€‹å­—ç¬¦ä¸²ï¼Œä»¥ä¾¿ TfidfVectorizer è™•ç†\n",
        "        filtered_words = [\n",
        "            word.strip()\n",
        "            for word in words\n",
        "            if word.strip() and len(word.strip()) > 1 and word.strip() not in stopwords\n",
        "        ]\n",
        "\n",
        "        # TfidfVectorizer éœ€è¦çš„æ˜¯å­—ä¸²å½¢å¼çš„æ–‡æª”\n",
        "        document = \" \".join(filtered_words)\n",
        "        document_list.append(document)\n",
        "\n",
        "# document_list ç¾åœ¨æ˜¯ä¸€å€‹åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ æ˜¯ç¶“éè™•ç†çš„æ¨™é¡Œå­—ä¸²\n",
        "# print(document_list)"
      ],
      "metadata": {
        "id": "TmNXPjV0ZslM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. åˆå§‹åŒ– TfidfVectorizer\n",
        "# TfidfVectorizer æœƒè™•ç†ï¼š\n",
        "#    a. å°‡æ–‡æª”è½‰æ›ç‚ºè©é »çŸ©é™£ (CountVectorizer çš„å·¥ä½œ)\n",
        "#    b. è¨ˆç®— TF-IDF æ¬Šé‡ (TfidfTransformer çš„å·¥ä½œ)\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# 2. é€²è¡Œæ“¬åˆå’Œè½‰æ› (Fit and Transform)\n",
        "# tfidf_matrix æ˜¯ä¸€å€‹ç¨€ç–çŸ©é™£ (sparse matrix)ï¼ŒåŒ…å«æ‰€æœ‰æ–‡æª”çš„ TF-IDF æ¬Šé‡\n",
        "tfidf_matrix = vectorizer.fit_transform(document_list)\n",
        "\n",
        "# 3. ç²å–æ‰€æœ‰è©å½™ (ç‰¹å¾µåç¨±)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# 4. å°‡ç¨€ç–çŸ©é™£è½‰æ›ç‚º NumPy é™£åˆ—ï¼Œæ–¹ä¾¿æŸ¥çœ‹æ¬Šé‡\n",
        "tfidf_array = tfidf_matrix.toarray()"
      ],
      "metadata": {
        "id": "SCUXqQAKZtcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å‰µå»ºä¸€å€‹å­—å…¸ä¾†å­˜å„²æ¯å€‹è©å½™åŠå…¶åœ¨æ‰€æœ‰æ–‡æª”ä¸­çš„ TF-IDF å¹³å‡æ¬Šé‡\n",
        "avg_tfidf_scores = defaultdict(float)\n",
        "\n",
        "# éæ­·æ‰€æœ‰æ–‡æª”çš„æ¬Šé‡\n",
        "for doc_weights in tfidf_array:\n",
        "    # éæ­·å–®ç¯‡æ–‡æª”ä¸­çš„æ‰€æœ‰è©å½™åŠå…¶æ¬Šé‡\n",
        "    for i, weight in enumerate(doc_weights):\n",
        "        word = feature_names[i]\n",
        "        avg_tfidf_scores[word] += weight\n",
        "\n",
        "# è¨ˆç®—å¹³å‡å€¼\n",
        "num_documents = len(document_list)\n",
        "for word in avg_tfidf_scores:\n",
        "    avg_tfidf_scores[word] /= num_documents\n",
        "\n",
        "# æŒ‰æ¬Šé‡é™åºæ’åˆ—\n",
        "sorted_avg_tfidf = sorted(avg_tfidf_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "print(\"--- æ•´å€‹æ–‡æª”é›†åˆä¸­è©å½™çš„ TF-IDF å¹³å‡æ¬Šé‡ (å‰ 10 å) ---\")\n",
        "for word, avg_weight in sorted_avg_tfidf[:10]:\n",
        "    print(f\"'{word}': {avg_weight:.4f}\")"
      ],
      "metadata": {
        "id": "-phQ9NdnZwYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============\n",
        "# PTT é›»å½±ç‰ˆçˆ¬èŸ²\n",
        "# ==============\n",
        "PTT_MOVIE_INDEX = \"https://www.ptt.cc/bbs/movie/index.html\"\n",
        "PTT_COOKIES = {\"over18\": \"1\"}\n",
        "\n",
        "def _get_soup(url):\n",
        "    r = requests.get(url, timeout=15, headers={\"User-Agent\":\"Mozilla/5.0\"}, cookies=PTT_COOKIES)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "def _get_prev_index_url(soup):\n",
        "    btns = soup.select(\"div.btn-group-paging a.btn.wide\")\n",
        "    # é é¢é€šå¸¸æœ‰ä¸‰å€‹ï¼šæœ€èˆŠ â† ä¸Šä¸€é  â† æœ€å‰é ï¼›å–ã€Œä¸Šä¸€é ã€\n",
        "    for a in btns:\n",
        "        if \"ä¸Šé \" in a.get_text(strip=True):\n",
        "            href = a.get(\"href\")\n",
        "            if href:\n",
        "                return \"https://www.ptt.cc\" + href\n",
        "    return None\n",
        "\n",
        "def _parse_nrec(nrec_span):\n",
        "    # å¯èƒ½æ˜¯æ•¸å­—æˆ– \"çˆ†\"/\"X1\" ç­‰\n",
        "    if not nrec_span:\n",
        "        return 0\n",
        "    txt = nrec_span.get_text(strip=True)\n",
        "    if txt == \"çˆ†\":\n",
        "        return 100\n",
        "    if txt.startswith(\"X\"):\n",
        "        try:\n",
        "            return -int(txt[1:])\n",
        "        except:\n",
        "            return -10\n",
        "    try:\n",
        "        return int(txt)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def _extract_post_list(soup):\n",
        "    posts = []\n",
        "    for r in soup.select(\"div.r-ent\"):\n",
        "        a = r.select_one(\"div.title a\")\n",
        "        if not a:\n",
        "            continue\n",
        "        title = a.get_text(strip=True)\n",
        "        url = \"https://www.ptt.cc\" + a.get(\"href\")\n",
        "        author = r.select_one(\"div.author\").get_text(strip=True)\n",
        "        date = r.select_one(\"div.date\").get_text(strip=True)\n",
        "        nrec = _parse_nrec(r.select_one(\"div.nrec span\"))\n",
        "        posts.append({\n",
        "            \"title\": title, \"url\": url, \"author\": author, \"date\": date, \"nrec\": nrec\n",
        "        })\n",
        "    return posts\n",
        "\n",
        "def _clean_ptt_content(soup):\n",
        "    # ç§»é™¤æ¨æ–‡å€\n",
        "    for p in soup.select(\"div.push\"):\n",
        "        p.decompose()\n",
        "    main = soup.select_one(\"#main-content\")\n",
        "    if not main:\n",
        "        return \"\", \"\"\n",
        "    # ç§»é™¤çœ‹æ¿çš„ meta è³‡è¨Šè¡Œï¼ˆä½œè€…/æ¨™é¡Œ/æ™‚é–“ï¼‰\n",
        "    metas = main.select(\"div.article-metaline, div.article-metaline-right\")\n",
        "    for m in metas:\n",
        "        m.decompose()\n",
        "    # å–å‡ºå…§æ–‡èˆ‡ç°½åæª”å‰åˆ‡å‰²\n",
        "    text = main.get_text(\"\\n\", strip=True)\n",
        "    if \"--\" in text:\n",
        "        text = text.split(\"--\")[0].strip()\n",
        "    # æ“·å–æ¨™é¡Œï¼ˆæœ‰äº›æ–‡ç« æ¨™é¡Œå¯å†è£œæ•‘ï¼‰\n",
        "    title_tag = soup.select_one(\"span.article-meta-value\")\n",
        "    meta_title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "    return text, meta_title\n",
        "\n",
        "def crawl_ptt_movie(index_pages=3, min_push=0, keyword=\"\"):\n",
        "    \"\"\"å¾æœ€æ–° index.html å¾€å‰ç¿» index_pages é ï¼ŒæŠ“æ»¿è¶³æ¢ä»¶çš„æ–‡ç« \"\"\"\n",
        "    global ptt_posts_df\n",
        "    url = PTT_MOVIE_INDEX\n",
        "    all_rows = []\n",
        "    seen_urls = set(ptt_posts_df[\"url\"].tolist()) if not ptt_posts_df.empty else set()\n",
        "\n",
        "    for _ in range(int(index_pages)):\n",
        "        soup = _get_soup(url)\n",
        "        posts = _extract_post_list(soup)\n",
        "        # ç¯©é¸\n",
        "        for p in posts:\n",
        "            if p[\"nrec\"] < int(min_push):\n",
        "                continue\n",
        "            if keyword and (keyword not in p[\"title\"]):\n",
        "                continue\n",
        "            # å»é‡ï¼ˆé¿å…åè¦†æŠ“åŒä¸€ç¯‡ï¼‰\n",
        "            if p[\"url\"] in seen_urls:\n",
        "                continue\n",
        "\n",
        "            # æŠ“æ­£æ–‡\n",
        "            try:\n",
        "                art_soup = _get_soup(p[\"url\"])\n",
        "                content, meta_title = _clean_ptt_content(art_soup)\n",
        "            except Exception as e:\n",
        "                content, meta_title = \"\", \"\"\n",
        "            final_title = p[\"title\"] if p[\"title\"] else (meta_title or \"ï¼ˆç„¡æ¨™é¡Œï¼‰\")\n",
        "\n",
        "            all_rows.append({\n",
        "                \"post_id\": str(uuid.uuid4())[:8],\n",
        "                \"title\": final_title[:200],\n",
        "                \"url\": p[\"url\"],\n",
        "                \"date\": p[\"date\"],\n",
        "                \"author\": p[\"author\"],\n",
        "                \"nrec\": str(p[\"nrec\"]),\n",
        "                \"created_at\": tznow().isoformat(),\n",
        "                \"fetched_at\": tznow().isoformat(),\n",
        "                \"content\": content\n",
        "            })\n",
        "\n",
        "        # æ›ä¸Šä¸€é \n",
        "        prev = _get_prev_index_url(soup)\n",
        "        if not prev:\n",
        "            break\n",
        "        url = prev\n",
        "\n",
        "    if all_rows:\n",
        "        new_df = pd.DataFrame(all_rows, columns=PTT_HEADER)\n",
        "        ptt_posts_df = pd.concat([ptt_posts_df, new_df], ignore_index=True)\n",
        "        write_df(ws_ptt_posts, ptt_posts_df, PTT_HEADER)\n",
        "        return f\"âœ… å–å¾— {len(all_rows)} ç¯‡æ–‡ç« ï¼ˆå·²å¯«å…¥ Sheetï¼‰\", ptt_posts_df\n",
        "    else:\n",
        "        return \"â„¹ï¸ æ²’æœ‰æ–°æ–‡ç« ç¬¦åˆæ¢ä»¶ï¼ˆæˆ–å…§å®¹å·²åœ¨ Sheetï¼‰\", ptt_posts_df\n",
        "\n",
        "\n",
        "# ==============\n",
        "# æ–‡æœ¬åˆ†æï¼ˆjieba + TF/IDF + bigramï¼‰\n",
        "# ==============\n",
        "import re\n",
        "try:\n",
        "    import jieba\n",
        "except:\n",
        "    jieba = None\n",
        "\n",
        "def _tokenize_zh(text):\n",
        "    text = re.sub(r\"[^\\u4e00-\\u9fffA-Za-z0-9]+\", \" \", text)\n",
        "    if not jieba:\n",
        "        # å¾Œå‚™ï¼šç”¨ç©ºç™½åˆ‡ï¼ˆè¼ƒå·®ï¼Œä½†é¿å…ç„¡æ³•åŸ·è¡Œï¼‰\n",
        "        return [t for t in text.split() if len(t) > 1]\n",
        "    return [w.strip() for w in jieba.lcut(text) if len(w.strip()) > 1]\n",
        "\n",
        "def analyze_ptt_texts(topk=50, min_df=2):\n",
        "    global ptt_posts_df, terms_df\n",
        "    if ptt_posts_df.empty:\n",
        "        return \"ğŸ“­ å°šç„¡å·²æŠ“å–çš„æ–‡ç« ï¼Œè«‹å…ˆåœ¨ã€PTT é›»å½±çˆ¬èŸ²ã€åˆ†é å–å¾—æ–‡ç« ã€‚\", terms_df, \"\"\n",
        "\n",
        "    docs = []\n",
        "    for _, r in ptt_posts_df.iterrows():\n",
        "        # å°‡æ¨™é¡Œèˆ‡å…§æ–‡æ‹¼èµ·ä¾†æé«˜é—œéµè©å¯è¦‹åº¦\n",
        "        docs.append((r[\"title\"] or \"\") + \"\\n\" + (r[\"content\"] or \"\"))\n",
        "\n",
        "    # è©é »\n",
        "    from collections import Counter, defaultdict\n",
        "    freq = Counter()\n",
        "    df_cnt = defaultdict(int)\n",
        "\n",
        "    token_docs = []\n",
        "    for doc in docs:\n",
        "        toks = _tokenize_zh(doc)\n",
        "        token_docs.append(toks)\n",
        "        freq.update(toks)\n",
        "        for t in set(toks):\n",
        "            df_cnt[t] += 1\n",
        "\n",
        "    # TF-IDFï¼ˆå¹³å‡å€¼ï¼‰\n",
        "    try:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        vec = TfidfVectorizer(tokenizer=_tokenize_zh, lowercase=False, min_df=min_df)\n",
        "        X = vec.fit_transform(docs)\n",
        "        terms = vec.get_feature_names_out()\n",
        "        tfidf_mean = X.mean(axis=0).A1\n",
        "        tfidf_map = dict(zip(terms, tfidf_mean))\n",
        "    except Exception:\n",
        "        tfidf_map = {t: 0.0 for t in freq.keys()}\n",
        "\n",
        "    # Bigramï¼ˆç²—ç•¥ï¼‰\n",
        "    from itertools import tee\n",
        "    def pairwise(iterable):\n",
        "        a, b = tee(iterable)\n",
        "        next(b, None)\n",
        "        return zip(a, b)\n",
        "    bigram_freq = Counter()\n",
        "    for toks in token_docs:\n",
        "        bigram_freq.update([\" \".join(bg) for bg in pairwise(toks)])\n",
        "\n",
        "    # å– TopK é—œéµè©ï¼ˆç¶œåˆï¼šå…ˆæŒ‰ tfidfï¼Œå†ç”¨ freq ç•¶æ¬¡æ’åºï¼‰\n",
        "    candidates = list(freq.keys())\n",
        "    candidates.sort(key=lambda t: (round(tfidf_map.get(t,0.0), 6), freq[t]), reverse=True)\n",
        "    top_terms = candidates[:int(topk)]\n",
        "\n",
        "    # ç¯„ä¾‹å¥ï¼ˆç°¡å–®å¾ä»»ä¸€ç¯‡å– 1 å€‹æ¨£ä¾‹ç‰‡æ®µï¼‰\n",
        "    examples = {}\n",
        "    for term in top_terms:\n",
        "        ex = \"\"\n",
        "        for doc in docs:\n",
        "            if term in doc:\n",
        "                # å–å‡ºåŒ…å«è©²è©çš„ç‰‡æ®µï¼ˆÂ±15å­—ï¼‰\n",
        "                i = doc.find(term)\n",
        "                s = max(0, i-15)\n",
        "                e = min(len(doc), i+len(term)+15)\n",
        "                ex = doc[s:e].replace(\"\\n\",\" \")\n",
        "                break\n",
        "        examples[term] = ex\n",
        "\n",
        "    rows = []\n",
        "    for t in top_terms:\n",
        "        rows.append({\n",
        "            \"term\": t,\n",
        "            \"freq\": str(freq[t]),\n",
        "            \"df_count\": str(df_cnt[t]),\n",
        "            \"tfidf_mean\": f\"{tfidf_map.get(t,0.0):.6f}\",\n",
        "            \"examples\": examples.get(t, \"\")\n",
        "        })\n",
        "    terms_df = pd.DataFrame(rows, columns=TERMS_HEADER)\n",
        "    write_df(ws_ptt_terms, terms_df, TERMS_HEADER)\n",
        "\n",
        "    # ç”¢ç”Ÿ Markdown æ‘˜è¦\n",
        "    md_lines = []\n",
        "    md_lines.append(f\"### é—œéµè© Top {len(top_terms)}ï¼ˆä¾ TF-IDF å¹³å‡å€¼å„ªå…ˆï¼Œæ¬¡åºå†ä»¥è©é »ï¼‰\")\n",
        "    for i, t in enumerate(top_terms, 1):\n",
        "        md_lines.append(f\"{i}. **{t}** â€” tfidfâ‰ˆ{float(tfidf_map.get(t,0.0)):.4f}ï¼›freq={freq[t]}ï¼›df={df_cnt[t]}\")\n",
        "    md_lines.append(\"\\n### å¸¸è¦‹é›™è©æ­é…ï¼ˆå‰ 20ï¼‰\")\n",
        "    for i, (bg, c) in enumerate(bigram_freq.most_common(20), 1):\n",
        "        md_lines.append(f\"{i}. {bg} â€” {c}\")\n",
        "\n",
        "    return f\"âœ… å·²å®Œæˆæ–‡æœ¬åˆ†æï¼Œå…± {len(docs)} ç¯‡æ–‡ç« ï¼›é—œéµè©å·²å¯«å…¥ Sheetã€‚\", terms_df, \"\\n\".join(md_lines)"
      ],
      "metadata": {
        "id": "XKJcaSD7UwoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_df(ws, header):\n",
        "    df = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(columns=header)\n",
        "    df = df.fillna(\"\")\n",
        "    # ä¿è­‰æ¬„ä½é½Šå…¨\n",
        "    for c in header:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\"\n",
        "    # å‹åˆ¥å¾®èª¿\n",
        "    if \"est_min\" in df.columns:\n",
        "        df[\"est_min\"] = pd.to_numeric(df[\"est_min\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    if \"actual_min\" in df.columns:\n",
        "        df[\"actual_min\"] = pd.to_numeric(df[\"actual_min\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    if \"pomodoros\" in df.columns:\n",
        "        df[\"pomodoros\"] = pd.to_numeric(df[\"pomodoros\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    return df[header]\n",
        "\n",
        "# çˆ¬èŸ²ï¼šæ“·å–æ–‡å­—æˆ–é€£çµä¸¦å¯åŠ å…¥ä»»å‹™\n",
        "# =========================\n",
        "def crawl(url, selector, mode, limit):\n",
        "    try:\n",
        "        resp = requests.get(url, timeout=15, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
        "        resp.raise_for_status()\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(columns=CLIPS_HEADER), f\"âš ï¸ è«‹æ±‚å¤±æ•—ï¼š{e}\"\n",
        "\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    nodes = soup.select(selector)\n",
        "    rows = []\n",
        "    for i, n in enumerate(nodes[:int(limit) if limit else 20]):\n",
        "        text = n.get_text(strip=True) if mode in (\"text\",\"both\") else \"\"\n",
        "        href = n.get(\"href\") if mode in (\"href\",\"both\") else \"\"\n",
        "        # ç›¸å°é€£çµè™•ç†\n",
        "        if href and href.startswith(\"/\"):\n",
        "            from urllib.parse import urljoin\n",
        "            href = urljoin(url, href)\n",
        "        rows.append({\n",
        "            \"clip_id\": str(uuid.uuid4())[:8],\n",
        "            \"url\": url,\n",
        "            \"selector\": selector,\n",
        "            \"text\": text,\n",
        "            \"href\": href,\n",
        "            \"created_at\": tznow().isoformat(),\n",
        "            \"added_to_task\": \"\"\n",
        "        })\n",
        "    df = pd.DataFrame(rows, columns=CLIPS_HEADER)\n",
        "    return df, f\"âœ… æ“·å– {len(df)} ç­†\"\n",
        "\n",
        "def add_clips_as_tasks(clip_ids, default_priority, est_min):\n",
        "    global clips_df, tasks_df\n",
        "    if not clip_ids:\n",
        "        return \"âš ï¸ è«‹å…ˆå‹¾é¸è¦åŠ å…¥çš„çˆ¬èŸ²é …ç›®\", clips_df, tasks_df\n",
        "    sel = clips_df[clips_df[\"clip_id\"].isin(clip_ids)]\n",
        "    _now = tznow().isoformat()\n",
        "    new_tasks = []\n",
        "    for _, r in sel.iterrows():\n",
        "        title = r[\"text\"] or r[\"href\"] or \"ï¼ˆæœªå‘½åï¼‰\"\n",
        "        note = f\"ä¾†æºï¼š{r['url']}\\né¸æ“‡å™¨ï¼š{r['selector']}\\né€£çµï¼š{r['href']}\"\n",
        "        new_tasks.append({\n",
        "            \"id\": str(uuid.uuid4())[:8],\n",
        "            \"task\": title[:120],\n",
        "            \"status\": \"todo\",\n",
        "            \"priority\": default_priority or \"M\",\n",
        "            \"est_min\": int(est_min) if est_min else 25,\n",
        "            \"start_time\": \"\",\n",
        "            \"end_time\": \"\",\n",
        "            \"actual_min\": 0,\n",
        "            \"pomodoros\": 0,\n",
        "            \"due_date\": \"\",\n",
        "            \"labels\": \"from:crawler\",\n",
        "            \"notes\": note,\n",
        "            \"created_at\": _now,\n",
        "            \"updated_at\": _now,\n",
        "            \"completed_at\": \"\",\n",
        "            \"planned_for\": \"\"\n",
        "        })\n",
        "    if new_tasks:\n",
        "        tasks_df = pd.concat([tasks_df, pd.DataFrame(new_tasks)], ignore_index=True)\n",
        "        # æ¨™è¨˜å·²åŠ å…¥\n",
        "        clips_df.loc[clips_df[\"clip_id\"].isin(clip_ids), \"added_to_task\"] = \"yes\"\n",
        "        write_df(ws_tasks, tasks_df, TASKS_HEADER)\n",
        "        write_df(ws_clips, clips_df, CLIPS_HEADER)\n",
        "        return f\"âœ… å·²åŠ å…¥ {len(new_tasks)} é …ç‚ºä»»å‹™\", clips_df, tasks_df\n",
        "    return \"âš ï¸ ç„¡å¯åŠ å…¥é …ç›®\", clips_df, tasks_df\n",
        "\n",
        "\n",
        "def read_ptt_posts_df():\n",
        "    return read_df(ws_ptt_posts, PTT_HEADER).copy()\n",
        "\n",
        "def read_terms_df():\n",
        "    return read_df(ws_ptt_terms, TERMS_HEADER).copy()\n",
        "\n",
        "ptt_posts_df = read_ptt_posts_df()\n",
        "terms_df = read_terms_df()\n"
      ],
      "metadata": {
        "id": "uCOjt-sDVK39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸ï¼šGoogle Sheet è®€å¯«ã€çˆ¬èŸ²æ·±åº¦ã€TF-IDFã€GeminiAPI\n",
        "1.æ–°å¢ï¼šè³‡æ–™è®€å¯«èˆ‡æ–‡ç« å…§å®¹çˆ¬å–å‡½æ•¸"
      ],
      "metadata": {
        "id": "IgpxkmEpW-ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_df(worksheet, df, header):\n",
        "    \"\"\"å°‡ DataFrame å¯«å…¥ Google Sheetï¼Œä¸¦ç¢ºä¿è¡¨é ­æ­£ç¢º\"\"\"\n",
        "    if df.empty:\n",
        "        return 0\n",
        "\n",
        "    # ç¢ºä¿ DataFrame åªæœ‰è¡¨é ­æŒ‡å®šçš„æ¬„ä½\n",
        "    data_to_write = [header] + df[header].fillna('').astype(str).values.tolist()\n",
        "\n",
        "    # è¨ˆç®—æ›´æ–°ç¯„åœ\n",
        "    num_rows = len(data_to_write)\n",
        "    num_cols = len(header)\n",
        "    range_label = f'A1:{chr(ord(\"A\") + num_cols - 1)}{num_rows}'\n",
        "\n",
        "    worksheet.clear()\n",
        "    worksheet.update(data_to_write, range_name=range_label)\n",
        "    return len(data_to_write) - 1\n",
        "\n",
        "def read_df(worksheet, header):\n",
        "    \"\"\"å¾ Google Sheet è®€å–è³‡æ–™ä¸¦è½‰æ›æˆ DataFrame\"\"\"\n",
        "    data = worksheet.get_all_values()\n",
        "    if not data or data[0] != header:\n",
        "        return pd.DataFrame([], columns=header)\n",
        "    # ç¬¬ä¸€è¡Œç‚ºè¡¨é ­ï¼Œå¾ç¬¬äºŒè¡Œé–‹å§‹æ‰æ˜¯è³‡æ–™\n",
        "    return pd.DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "def get_article_content(article_url):\n",
        "    \"\"\"ç²å–å–®ä¸€ PTT æ–‡ç« çš„å…§å®¹å’Œè©³ç´°è³‡è¨Š\"\"\"\n",
        "    if \"N/A\" in article_url:\n",
        "        return \"N/A\", \"N/A\", \"N/A\"\n",
        "    try:\n",
        "        res = requests.get(article_url, headers=headers, timeout=5)\n",
        "        soup = BeautifulSoup(res.text, 'html.parser')\n",
        "        main_content = soup.find(id=\"main-content\")\n",
        "\n",
        "        # ç§»é™¤æ–‡ç« è³‡è¨Š (meta lines) å’Œæ¨æ–‡å€\n",
        "        for tag in main_content.find_all('div', class_='article-metaline'):\n",
        "            tag.extract()\n",
        "        for tag in main_content.find_all('div', class_='article-metaline-right'):\n",
        "            tag.extract()\n",
        "        for push in main_content.find_all('div', class_='push'):\n",
        "            push.extract()\n",
        "\n",
        "        # æå–å‰©ä¸‹çš„æ–‡å­—å…§å®¹\n",
        "        content = main_content.text.strip()\n",
        "\n",
        "        # è™•ç† post_id, created_at (å¾ç¶²å€ä¸­æå– Unix Timestamp)\n",
        "        post_id_match = re.search(r'M\\.(\\d+)\\.A\\.', article_url)\n",
        "        post_id = post_id_match.group(1) if post_id_match else uuid.uuid4().hex\n",
        "\n",
        "        try:\n",
        "            created_at_ts = int(post_id)\n",
        "            # å°‡ Unix Timestamp è½‰æ›ç‚º ISO æ ¼å¼æ™‚é–“\n",
        "            created_at = dt.fromtimestamp(created_at_ts, tz=gettz(TIMEZONE)).isoformat()\n",
        "        except Exception:\n",
        "            created_at = dt.now(gettz(TIMEZONE)).isoformat()\n",
        "\n",
        "        return post_id, content, created_at\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching article {article_url}: {e}\")\n",
        "        return uuid.uuid4().hex, \"N/A\", dt.now(gettz(TIMEZONE)).isoformat()"
      ],
      "metadata": {
        "id": "o8WvPY1oVlHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.æ–°å¢ï¼šTF-IDF é—œéµå­—åˆ†æå‡½æ•¸"
      ],
      "metadata": {
        "id": "tD81nQnxXJBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tfidf(posts_df, top_n=20):\n",
        "    \"\"\"\n",
        "    åŸ·è¡Œä¸­æ–‡æ–·è©å’Œ TF-IDF è¨ˆç®—ï¼Œæ‰¾å‡ºå‰ N ç†±è©ã€‚\n",
        "    \"\"\"\n",
        "    # ç¢ºä¿åªåˆ†ææœ‰å…§å®¹çš„è²¼æ–‡\n",
        "    posts_df = posts_df[posts_df['content'].str.strip() != 'N/A'].copy()\n",
        "    if posts_df.empty:\n",
        "        # å¦‚æœæ²’æœ‰æœ‰æ•ˆè³‡æ–™ï¼Œå›å‚³ç©ºçš„ DataFrame\n",
        "        return pd.DataFrame([], columns=TERMS_HEADER)\n",
        "\n",
        "    # é€²è¡Œä¸­æ–‡æ–·è©ï¼šä½¿ç”¨ Jieba é è¨­è©å…¸\n",
        "    # å·²ç§»é™¤ jieba.set_dictionary(\"dict.txt.big\") ä»¥é¿å…æª”æ¡ˆä¸å­˜åœ¨çš„éŒ¯èª¤\n",
        "    posts_df['segmented_content'] = posts_df['content'].apply(\n",
        "        lambda x: \" \".join(jieba.cut(x.replace('\\n', ' ').replace('\\r', ''), cut_all=False))\n",
        "    )\n",
        "\n",
        "    # TF-IDF è¨­å®š (ç§»é™¤ç´”æ•¸å­—ã€è‹±æ–‡å–®è©ç­‰)\n",
        "    tfidf = TfidfVectorizer(\n",
        "        stop_words=[],\n",
        "        token_pattern=r\"[\\u4e00-\\u9fa5]{2,}\", # è‡³å°‘å…©å€‹ä¸­æ–‡å­—çš„è©å½™\n",
        "        min_df=5, # è‡³å°‘åœ¨ 5 ç¯‡æ–‡ç« ä¸­å‡ºç¾\n",
        "    )\n",
        "\n",
        "    # éæ¿¾æ‰åˆ†è©å¾Œç‚ºç©ºå­—ä¸²çš„åˆ—ï¼Œé¿å… fit_transform å¤±æ•—\n",
        "    valid_docs = posts_df[posts_df['segmented_content'].str.strip() != '']['segmented_content']\n",
        "    if valid_docs.empty:\n",
        "        return pd.DataFrame([], columns=TERMS_HEADER)\n",
        "\n",
        "    tfidf_matrix = tfidf.fit_transform(valid_docs)\n",
        "    feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "    # 1. è¨ˆç®—å¹³å‡ TF-IDF åˆ†æ•¸ (ä½œç‚ºæ¬Šé‡)\n",
        "    avg_tfidf_scores = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
        "    df_results = pd.DataFrame({'term': feature_names, 'tfidf_mean': avg_tfidf_scores})\n",
        "\n",
        "    # 2. è¨ˆç®—æ–‡ä»¶é »ç‡ (df_count)\n",
        "    df_count_map = defaultdict(int)\n",
        "    for doc in valid_docs:\n",
        "        # åªè¨ˆç®—åœ¨æ­¤æ¬¡ TF-IDF åˆ†æä¸­è¢«æ¡ç”¨çš„è©å½™\n",
        "        unique_words = set(re.findall(tfidf.token_pattern, doc))\n",
        "        for word in unique_words:\n",
        "            df_count_map[word] += 1\n",
        "\n",
        "    df_count = pd.DataFrame(df_count_map.items(), columns=['term', 'df_count'])\n",
        "\n",
        "    # 3. è¨ˆç®—ç¸½è©é » (freq)\n",
        "    word_counts = defaultdict(int)\n",
        "    for doc in valid_docs:\n",
        "        for word in re.findall(tfidf.token_pattern, doc):\n",
        "            word_counts[word] += 1\n",
        "\n",
        "    df_freq = pd.DataFrame(word_counts.items(), columns=['term', 'freq'])\n",
        "\n",
        "    # åˆä½µçµæœ\n",
        "    df_combined = pd.merge(df_results, df_count, on='term', how='inner')\n",
        "    df_combined = pd.merge(df_combined, df_freq, on='term', how='inner')\n",
        "\n",
        "    # æ‰¾å‡º top N ç†±è©\n",
        "    top_terms = df_combined.sort_values(by='tfidf_mean', ascending=False).head(top_n)\n",
        "\n",
        "    # æå–ä¾‹å­ (å¾æ–‡ç« æ¨™é¡Œ)\n",
        "    example_map = {}\n",
        "    # ä½¿ç”¨ posts_dfï¼Œå› ç‚ºå®ƒåŒ…å« 'title' æ¬„ä½\n",
        "    posts_for_examples = posts_df[posts_df['segmented_content'].str.strip() != '']\n",
        "\n",
        "    for term in top_terms['term']:\n",
        "        # æ‰¾å‡ºåŒ…å«è©²è©å½™çš„æ–‡ç« æ¨™é¡Œ (æœ€å¤š 3 å€‹)\n",
        "        examples = posts_for_examples[posts_for_examples['segmented_content'].str.contains(r'\\b' + re.escape(term) + r'\\b', na=False)]['title'].head(3).tolist()\n",
        "        example_map[term] = \" / \".join(examples)\n",
        "\n",
        "    top_terms['examples'] = top_terms['term'].map(example_map)\n",
        "\n",
        "    return top_terms[TERMS_HEADER] # æŒ‰ç…§è¡¨é ­é †åºå›å‚³"
      ],
      "metadata": {
        "id": "CoIvLsMVXQaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.æ–°å¢ï¼šGemini API æ´å¯Ÿç”Ÿæˆå‡½æ•¸"
      ],
      "metadata": {
        "id": "lFRHIRm9XUbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "\n",
        "def generate_insights(top_terms_df, model):\n",
        "    \"\"\"\n",
        "    ä¸²æ¥ Gemini API æ ¹æ“šç†±è©çµæœç”Ÿæˆæ´å¯Ÿæ‘˜è¦å’Œçµè«–ï¼Œä¸¦å¢åŠ å¥å£¯æ€§æª¢æŸ¥ã€‚\n",
        "    \"\"\"\n",
        "    if top_terms_df.empty:\n",
        "        return \"âš ï¸ è³‡æ–™ä¸è¶³ï¼šTF-IDF åˆ†æçµæœç‚ºç©ºï¼Œç„¡æ³•ç”Ÿæˆæ´å¯Ÿæ‘˜è¦å’Œçµè«–ã€‚\"\n",
        "\n",
        "    # æº–å‚™æç¤ºè©\n",
        "    top_terms_list = top_terms_df.to_dict('records')\n",
        "    # é€™è£¡åªå–éœ€è¦çš„æ¬„ä½ä¾†çµ„è£æç¤ºè©ï¼Œé¿å…å‚³éä¸å¿…è¦çš„è³‡è¨Š\n",
        "    formatted_terms = \"\\n\".join([\n",
        "        f\"- {item['term']} (å¹³å‡ TF-IDF: {float(item['tfidf_mean']):.4f}, ç¸½é »ç‡: {item['freq']})\"\n",
        "        for item in top_terms_list\n",
        "    ])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å¸‚å ´åˆ†æå¸«ã€‚è«‹æ ¹æ“šä»¥ä¸‹ PTT Movie æ¿çš„ç†±è©åˆ†æçµæœï¼Œç‚ºè¿‘æœŸè¨è«–ç”¢å‡ºå°ˆæ¥­çš„æ´å¯Ÿæ‘˜è¦èˆ‡çµè«–ã€‚\n",
        "\n",
        "    ç†±è©åˆ†æçµæœ (Top {len(top_terms_list)}):\n",
        "    {formatted_terms}\n",
        "\n",
        "    è«‹åš´æ ¼ä»¥ç¹é«”ä¸­æ–‡å®Œæˆä»¥ä¸‹è¦æ±‚ï¼Œä¸¦ç›´æ¥è¼¸å‡ºçµæœï¼Œä¸åŒ…å«ä»»ä½•é¡å¤–è§£é‡‹ï¼š\n",
        "    1.  **5 å¥æ´å¯Ÿæ‘˜è¦**ï¼šç¸½çµç›®å‰çš„è¨è«–è¶¨å‹¢ã€ç†±é–€è©±é¡Œæˆ–ç”¨æˆ¶æƒ…ç·’ï¼Œæ¯å¥ç¨ç«‹æˆæ®µï¼Œå¥é¦–åŠ ä¸Šã€Œæ´å¯ŸXï¼šã€ã€‚\n",
        "    2.  **ä¸€æ®µ 120 å­—çµè«–**ï¼šç”¨ä¸€æ®µè©±ç¸½çµæ•´é«”æƒ…æ³ï¼Œä¸¦æä¾›ä¸€å€‹æœªä¾†è§€å¯Ÿå»ºè­°ã€‚çµè«–å­—æ•¸ç´„åœ¨ 100-140 å­—ä¹‹é–“ã€‚\n",
        "\n",
        "    è¼¸å‡ºæ ¼å¼ç¯„ä¾‹:\n",
        "    æ´å¯Ÿ1ï¼š...\n",
        "    æ´å¯Ÿ2ï¼š...\n",
        "    æ´å¯Ÿ3ï¼š...\n",
        "    æ´å¯Ÿ4ï¼š...\n",
        "    æ´å¯Ÿ5ï¼š...\n",
        "\n",
        "    çµè«–ï¼š... (ç´„120å­—)\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "\n",
        "        # --- é—œéµä¿®æ­£é»ï¼šæª¢æŸ¥å›è¦†æ˜¯å¦è¢«å®‰å…¨éæ¿¾é˜»æ“‹ ---\n",
        "        # å¦‚æœ candidates åˆ—è¡¨ç‚ºç©ºï¼Œé€šå¸¸è¡¨ç¤ºå›æ‡‰è¢«é˜»æ“‹\n",
        "        if not response.candidates:\n",
        "             # å˜—è©¦ç²å– safety rating\n",
        "            safety_reason = \"æœªçŸ¥åŸå› \"\n",
        "            if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                safety_reason = f\"Block Reason: {response.prompt_feedback.block_reason.name}\"\n",
        "\n",
        "            return f\"âš ï¸ Gemini API å‘¼å«å¤±æ•—ï¼šæ¨¡å‹å›è¦†å› å…§å®¹æ”¿ç­–è¢«é˜»æ“‹ã€‚{safety_reason}ã€‚è«‹æª¢æŸ¥æ‚¨çš„ç†±è©å…§å®¹ã€‚\"\n",
        "\n",
        "        generated_text = response.text.strip()\n",
        "\n",
        "        if not generated_text:\n",
        "            return \"âš ï¸ Gemini API å‘¼å«å¤±æ•—ï¼šæ¨¡å‹å›è¦†å…§å®¹ç‚ºç©ºã€‚è«‹ç¢ºä¿æç¤ºè©å…§å®¹è¶³å¤ è±å¯Œã€‚\"\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        # æ•ç²ç¶²è·¯éŒ¯èª¤ã€API é‡‘é‘°éŒ¯èª¤æˆ–å…¶ä»–æœªé æœŸéŒ¯èª¤\n",
        "        return f\"âš ï¸ Gemini API å‘¼å«å¤±æ•— (æœªé æœŸéŒ¯èª¤): {type(e).__name__}: {e}\""
      ],
      "metadata": {
        "id": "ZwtRjxY2XYZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# æ•´åˆç‚ºè‡ªå‹•åŒ–æµç¨‹ï¼ˆGradio ä»‹é¢ï¼‰\n",
        "å°‡çˆ¬èŸ²ã€åˆ†æã€ç”Ÿæˆçµæœçš„åŠŸèƒ½æ•´åˆåˆ°ä¸€å€‹ä¸»è¦çš„ run_analysis_pipeline å‡½æ•¸ä¸­ã€‚"
      ],
      "metadata": {
        "id": "sl1V8WrSXmPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crawl_and_save_data(start_index, pages_to_fetch, ws_posts):\n",
        "    \"\"\"\n",
        "    å¾ PTT çˆ¬å–å¤šé æ–‡ç« æ¨™é¡Œï¼Œä¸¦æ·±å…¥çˆ¬å–æ–‡ç« å…§å®¹å¾Œå¯«å…¥ Google Sheetã€‚\n",
        "    \"\"\"\n",
        "    BASE_URL = \"https://www.ptt.cc/bbs/movie/index\"\n",
        "    articles_data = []\n",
        "\n",
        "    stop_index = start_index - pages_to_fetch\n",
        "    fetched_at = dt.now(gettz(TIMEZONE)).isoformat()\n",
        "\n",
        "    for index in range(start_index, stop_index, -1):\n",
        "        url = f\"{BASE_URL}{index}.html\"\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=5)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            article_list = soup.find_all('div', class_='r-ent')\n",
        "\n",
        "            for article in article_list:\n",
        "                title_tag = article.find('div', class_='title').find('a')\n",
        "\n",
        "                if title_tag:\n",
        "                    title = title_tag.text.strip()\n",
        "                    href = \"https://www.ptt.cc\" + title_tag['href']\n",
        "                else:\n",
        "                    title = article.find('div', class_='title').text.strip()\n",
        "                    href = \"N/A (å·²åˆªé™¤æˆ–ä¸å¯å­˜å–)\"\n",
        "\n",
        "                author = article.find('div', class_='author').text.strip()\n",
        "                date = article.find('div', class_='date').text.strip()\n",
        "\n",
        "                # æ·±åº¦çˆ¬å–æ–‡ç« å…§å®¹\n",
        "                post_id, content, created_at = get_article_content(href)\n",
        "\n",
        "                articles_data.append({\n",
        "                    'post_id': post_id,\n",
        "                    'title': title,\n",
        "                    'url': href,\n",
        "                    'date': date,\n",
        "                    'author': author,\n",
        "                    'nrec': article.find('div', class_='nrec').text.strip(), # è£œä¸Šç†±åº¦\n",
        "                    'created_at': created_at,\n",
        "                    'fetched_at': fetched_at,\n",
        "                    'content': content\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching page {url}: {e}\")\n",
        "            continue\n",
        "\n",
        "    df_posts = pd.DataFrame(articles_data)\n",
        "    posts_written = write_df(ws_posts, df_posts, PTT_HEADER)\n",
        "    return df_posts, posts_written\n",
        "\n",
        "def run_analysis_pipeline(pages_to_fetch):\n",
        "    \"\"\"\n",
        "    è‡ªå‹•åŒ–æµç¨‹ä¸»å‡½æ•¸ï¼šçˆ¬èŸ² -> Sheet -> TF-IDF -> Gemini -> é¡¯ç¤ºçµæœ\n",
        "    \"\"\"\n",
        "    # ç¢ºä¿ Google Sheet é€£ç·šå’Œå·¥ä½œè¡¨å­˜åœ¨\n",
        "    try:\n",
        "        sh = gc.open(WORKSHEET_NAME)\n",
        "        ws_ptt_posts = sh.worksheet(\"ptt_movie_posts\")\n",
        "        ws_ptt_terms = sh.worksheet(\"ptt_movie_terms\")\n",
        "    except Exception:\n",
        "        return \"âš ï¸ éŒ¯èª¤ï¼šè«‹ç¢ºä¿ Google Sheets èªè­‰å·²å®Œæˆï¼Œä¸”å·¥ä½œè¡¨åç¨±æ­£ç¢ºã€‚\", \"\", \"\", \"\"\n",
        "\n",
        "    # 1. çˆ¬èŸ²çµæœ â†’ å¯«å…¥ Google Sheet\n",
        "    output_message = \"--- é–‹å§‹åŸ·è¡Œè‡ªå‹•åŒ–æµç¨‹ ---\\n\"\n",
        "\n",
        "    # ç²å–èµ·å§‹é ç¢¼ (ä½¿ç”¨æ‚¨åŸæœ‰çš„é‚è¼¯)\n",
        "    responseIndex = requests.get(\"https://www.ptt.cc/bbs/movie/index.html\", headers=headers, timeout=5)\n",
        "    soupIndex = BeautifulSoup(responseIndex.text, 'html.parser')\n",
        "    current_url = get_previous_page_url(soupIndex)\n",
        "    start_index = extract_index_split(current_url)\n",
        "\n",
        "    df_posts, posts_written = crawl_and_save_data(start_index, pages_to_fetch, ws_ptt_posts)\n",
        "    output_message += f\"âœ… 1. çˆ¬èŸ²å®Œæˆï¼šæˆåŠŸçˆ¬å– {posts_written} ç¯‡æ–‡ç« ï¼Œå·²å¯«å…¥ã€Œptt_movie_postsã€å·¥ä½œè¡¨ã€‚\\n\"\n",
        "\n",
        "    # 2. å¾ Sheet è®€å–è³‡æ–™ â†’ è©é »èˆ‡é—œéµå­—çµ±è¨ˆï¼ˆTF-IDFï¼‰\n",
        "    # é€™è£¡ç›´æ¥ä½¿ç”¨å‰›çˆ¬å–çš„ df_posts é€²è¡Œåˆ†æï¼Œé¿å…å†æ¬¡è®€å– Sheet\n",
        "    top_n = 20 # é è¨­è¼¸å‡ºå‰ 20 ç†±è©\n",
        "    df_terms = calculate_tfidf(df_posts, top_n=top_n)\n",
        "\n",
        "    output_message += f\"âœ… 2. TF-IDF åˆ†æå®Œæˆï¼šå·²å¾ {len(df_posts)} ç¯‡è²¼æ–‡è¨ˆç®—å‡º Top {top_n} ç†±è©ã€‚\\n\"\n",
        "\n",
        "    # 3. è¼¸å‡ºå‰ N ç†±è© â†’ å›å¯«åˆ°çµ±è¨ˆè¡¨\n",
        "    terms_written = write_df(ws_ptt_terms, df_terms, TERMS_HEADER)\n",
        "    output_message += f\"âœ… 3. ç†±è©å›å¯«å®Œæˆï¼šå·²å°‡ {terms_written} æ¢ç†±è©å¯«å…¥ã€Œptt_movie_termsã€å·¥ä½œè¡¨ã€‚\\n\"\n",
        "\n",
        "    # 4. ä¸²æ¥ Gemini API ç”Ÿæˆæ´å¯Ÿæ‘˜è¦èˆ‡çµè«–\n",
        "    insight_output = generate_insights(df_terms, model)\n",
        "    output_message += \"âœ… 4. Gemini API æ´å¯Ÿç”Ÿæˆå®Œæˆã€‚\\n\"\n",
        "    output_message += \"---------------------------\\n\"\n",
        "    output_message += \"âœ¨ æ´å¯Ÿæ‘˜è¦èˆ‡çµè«–å·²æº–å‚™å¥½ã€‚\"\n",
        "\n",
        "    # æ•´ç†çµæœè¼¸å‡º\n",
        "    # ç†±è©çµæœè½‰ç‚º Markdown è¡¨æ ¼\n",
        "    terms_markdown = df_terms[['term', 'freq', 'tfidf_mean', 'examples']].to_markdown(index=False, floatfmt=\".4f\")\n",
        "\n",
        "    return output_message, insight_output, terms_markdown, SHEET_URL.replace(\"/edit?usp=sharing\", \"/edit\")\n",
        "\n",
        "# å‡è¨­æ‚¨åŸæœ‰çš„è¼”åŠ©å‡½æ•¸ (get_previous_page_url, extract_index_split) å·²ä¿ç•™\n",
        "def get_previous_page_url(soup):\n",
        "    paging_div = soup.find('div', class_='btn-group btn-group-paging')\n",
        "    if paging_div:\n",
        "        prev_button = paging_div.find_all('a')[1]\n",
        "        if 'href' in prev_button.attrs:\n",
        "            return \"https://www.ptt.cc\" + prev_button['href']\n",
        "    return None\n",
        "\n",
        "def extract_index_split(url):\n",
        "    try:\n",
        "        index_str = url.split('index')[1].split('.html')[0]\n",
        "        return int(index_str)\n",
        "    except Exception:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "Y3GoiI14Xmo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. æ•´åˆæˆ Gradio ä»‹é¢\n",
        "with gr.Blocks(title=\"æ–‡å­—è³‡æ–™åˆ†æè‡ªå‹•åŒ–æµç¨‹ (PTT Movie)\") as demo:\n",
        "    gr.Markdown(\"## ğŸ¬ PTT Movie æ–‡å­—è³‡æ–™è‡ªå‹•åŒ–åˆ†æå™¨\")\n",
        "    gr.Markdown(\"å¯ä¸€éµåŸ·è¡Œï¼š**çˆ¬èŸ²** (è¿‘ N é ) â†’ **Google Sheet å¯«å…¥** â†’ **TF-IDF åˆ†æ** â†’ **ç†±è©å›å¯«** â†’ **Gemini æ´å¯Ÿæ‘˜è¦/çµè«–ç”Ÿæˆ**ã€‚\")\n",
        "\n",
        "    with gr.Row():\n",
        "        pages_input = gr.Slider(\n",
        "            minimum=1,\n",
        "            maximum=50,\n",
        "            step=1,\n",
        "            value=10,\n",
        "            label=\"æ¬²çˆ¬å–çš„é æ•¸ (æ¯é ç´„ 20 ç¯‡)\"\n",
        "        )\n",
        "\n",
        "        run_button = gr.Button(\"ğŸš€ ä¸€éµåŸ·è¡Œè‡ªå‹•åŒ–åˆ†æ\", variant=\"primary\")\n",
        "\n",
        "    status_output = gr.Textbox(label=\"æµç¨‹ç‹€æ…‹èˆ‡è¨Šæ¯\", lines=4)\n",
        "    sheet_link_output = gr.HTML(label=\"Google Sheet é€£çµ\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### ğŸ’ Gemini AI æ´å¯Ÿæ‘˜è¦èˆ‡çµè«–\")\n",
        "            ai_output = gr.Textbox(label=\"åˆ†æçµæœ\", lines=15, interactive=False)\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### ğŸ”¥ TF-IDF Top ç†±è©çµ±è¨ˆçµæœ\")\n",
        "            terms_table = gr.Markdown(label=\"ç†±è©çµæœ (å·²å›å¯«è‡³ Google Sheet)\", value=\"ç­‰å¾…åŸ·è¡Œ...\")\n",
        "\n",
        "\n",
        "    def pipeline_wrapper(pages):\n",
        "        output, insights, terms_md, sheet_url = run_analysis_pipeline(pages)\n",
        "        link_html = f'<a href=\"{sheet_url}\" target=\"_blank\" style=\"color: blue; text-decoration: underline;\">ğŸ”— é»æ­¤é–‹å•Ÿ Google Sheet æŸ¥çœ‹åŸå§‹èˆ‡çµ±è¨ˆè³‡æ–™</a>'\n",
        "        return output, insights, terms_md, link_html\n",
        "\n",
        "    run_button.click(\n",
        "        fn=pipeline_wrapper,\n",
        "        inputs=[pages_input],\n",
        "        outputs=[status_output, ai_output, terms_table, sheet_link_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KrjIN4hdXxS-",
        "outputId": "7a90648c-0bfa-4a73-b536-d68aaae9a9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9ff16c3548a55a29f7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://9ff16c3548a55a29f7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2116, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1623, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 976, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 915, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-430219034.py\", line 31, in pipeline_wrapper\n",
            "    output, insights, terms_md, sheet_url = run_analysis_pipeline(pages)\n",
            "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2552998066.py\", line 80, in run_analysis_pipeline\n",
            "    df_terms = calculate_tfidf(df_posts, top_n=top_n)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4239570235.py\", line 11, in calculate_tfidf\n",
            "    jieba.set_dictionary(\"dict.txt.big\") # å‡è¨­æ‚¨æœ‰å¤§è©å…¸\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jieba/__init__.py\", line 513, in set_dictionary\n",
            "    raise Exception(\"jieba: file does not exist: \" + abs_path)\n",
            "Exception: jieba: file does not exist: /content/dict.txt.big\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2116, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1623, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 976, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 915, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-430219034.py\", line 31, in pipeline_wrapper\n",
            "    output, insights, terms_md, sheet_url = run_analysis_pipeline(pages)\n",
            "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2552998066.py\", line 80, in run_analysis_pipeline\n",
            "    df_terms = calculate_tfidf(df_posts, top_n=top_n)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4239570235.py\", line 11, in calculate_tfidf\n",
            "    jieba.set_dictionary(\"dict.txt.big\") # å‡è¨­æ‚¨æœ‰å¤§è©å…¸\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jieba/__init__.py\", line 513, in set_dictionary\n",
            "    raise Exception(\"jieba: file does not exist: \" + abs_path)\n",
            "Exception: jieba: file does not exist: /content/dict.txt.big\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2116, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1623, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 976, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 915, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-430219034.py\", line 31, in pipeline_wrapper\n",
            "    output, insights, terms_md, sheet_url = run_analysis_pipeline(pages)\n",
            "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2552998066.py\", line 80, in run_analysis_pipeline\n",
            "    df_terms = calculate_tfidf(df_posts, top_n=top_n)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4239570235.py\", line 11, in calculate_tfidf\n",
            "    jieba.set_dictionary(\"dict.txt.big\") # å‡è¨­æ‚¨æœ‰å¤§è©å…¸\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jieba/__init__.py\", line 513, in set_dictionary\n",
            "    raise Exception(\"jieba: file does not exist: \" + abs_path)\n",
            "Exception: jieba: file does not exist: /content/dict.txt.big\n"
          ]
        }
      ]
    }
  ]
}