{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPveRoU/ZWg2JB7npv7fv7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiangtinhung-rgb/Matcha/blob/main/HW4_%E6%96%87%E5%AD%97%E8%B3%87%E6%96%99%E5%B0%8F%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW4-文字資料小分析\n",
        "目標：從 Sheet 讀開放式回答 → 做詞數與關鍵字計數 → 輸出前 N 熱詞 → 回寫統計表。\n",
        "\n",
        "AI 點子：請模型產出 5 句洞察摘要 + 一段 120 字結論。\n"
      ],
      "metadata": {
        "id": "2SdUJNtVXA7G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvZ_3VmWWmg8"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "               gradio pandas beautifulsoup4 google-generativeai python-dateutil"
      ],
      "metadata": {
        "id": "IxdXRAuYMure"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 新增 TF-IDF 相關套件\n",
        "import jieba\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "o9araqEtWwdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, uuid, re, json, datetime\n",
        "from datetime import datetime as dt, timedelta\n",
        "from dateutil.tz import gettz\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Google Auth & Sheets\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2 import service_account\n",
        "from google.auth import default"
      ],
      "metadata": {
        "id": "BzthwvSkTDVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "ASYrK7EYTIPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# 從 Colab Secrets 中獲取 API 金鑰\n",
        "api_key = userdata.get('MATCHA')\n",
        "\n",
        "# 使用獲取的金鑰配置 genai\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.5-pro')"
      ],
      "metadata": {
        "id": "MhbbqXUVTJSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHEET_URL = \"https://docs.google.com/spreadsheets/d/1qR5tsLMzL1tVHRQvkg3gHrznzEWlVGIunhS-I7m9rzU/edit?usp=sharing\"\n",
        "WORKSHEET_NAME = \"工作表1\"\n",
        "TIMEZONE = \"Asia/Taipe\""
      ],
      "metadata": {
        "id": "zNib2vWhTPRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PTT_HEADER = [\n",
        "    \"post_id\",\"title\",\"url\",\"date\",\"author\",\"nrec\",\"created_at\",\n",
        "    \"fetched_at\",\"content\"\n",
        "]\n",
        "TERMS_HEADER = [\"term\",\"freq\",\"df_count\",\"tfidf_mean\",\"examples\"]"
      ],
      "metadata": {
        "id": "Cz3whVbaU_WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_spreadsheet(name):\n",
        "    try:\n",
        "        sh = gc.open(name)  # returns gspread.models.Spreadsheet\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        sh = gc.create(name)\n",
        "    return sh\n",
        "\n",
        "sh = ensure_spreadsheet(WORKSHEET_NAME)"
      ],
      "metadata": {
        "id": "sHGSAmywUS9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_worksheet(sh, title, header):\n",
        "    try:\n",
        "        ws = sh.worksheet(title)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=title, rows=\"1000\", cols=str(len(header)+5))\n",
        "        ws.update([header])\n",
        "    # 若沒有表頭就補上\n",
        "    data = ws.get_all_values()\n",
        "    if not data or (data and data[0] != header):\n",
        "        ws.clear()\n",
        "        ws.update([header])\n",
        "    return ws"
      ],
      "metadata": {
        "id": "l-9sJPlLUYP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ws_ptt_posts = ensure_worksheet(sh, \"ptt_movie_posts\", PTT_HEADER)\n",
        "ws_ptt_terms = ensure_worksheet(sh, \"ptt_movie_terms\", TERMS_HEADER)"
      ],
      "metadata": {
        "id": "lz-_uk8kUZVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_data = []"
      ],
      "metadata": {
        "id": "XYgnQHfiXrNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}"
      ],
      "metadata": {
        "id": "hoSHiN0VXuJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_previous_page_url(soup):\n",
        "    \"\"\"獲取上一頁 (更新、更舊的文章) 的連結\"\"\"\n",
        "    # PTT 的「上一頁」按鈕 HTML 結構：div class=\"btn-group btn-group-paging\"\n",
        "    paging_div = soup.find('div', class_='btn-group btn-group-paging')\n",
        "    if paging_div:\n",
        "        # PTT 頁面按鈕順序：最舊 (0), 上頁 (1), 下頁 (2), 最新 (3)\n",
        "        # 我們要找的是「上一頁」 (即更舊的文章) 的連結，索引為 1\n",
        "        prev_button = paging_div.find_all('a')[1]\n",
        "\n",
        "        # 檢查連結是否有效 (如果已經是第一頁，連結會是 '#' 或沒有 href)\n",
        "        if 'href' in prev_button.attrs:\n",
        "            return \"https://www.ptt.cc\" + prev_button['href']\n",
        "    return None"
      ],
      "metadata": {
        "id": "SJdcDWlgtzED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responseIndex = requests.get(\"https://www.ptt.cc/bbs/movie/index.html\", headers=headers, timeout=5)\n",
        "html_contentIndex = responseIndex.text\n",
        "soupIndex = BeautifulSoup(html_contentIndex, 'html.parser')"
      ],
      "metadata": {
        "id": "gsVSz5F2P1Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_url = get_previous_page_url(soupIndex)"
      ],
      "metadata": {
        "id": "QwW99_jwP75D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_url"
      ],
      "metadata": {
        "id": "x1K4uTfPQVzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_index_split(url):\n",
        "    \"\"\"\n",
        "    使用字串分割方法解析頁碼。\n",
        "\n",
        "    邏輯：\n",
        "    1. 以 'index' 分割網址：['...', '10808.html']\n",
        "    2. 取第二個元素 ('10808.html')\n",
        "    3. 以 '.html' 分割：['10808', '']\n",
        "    4. 取第一個元素 ('10808')\n",
        "    5. 轉換為整數\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 確保網址中包含 'index' 和 '.html'\n",
        "        index_str = url.split('index')[1].split('.html')[0]\n",
        "        return int(index_str)\n",
        "    except IndexError:\n",
        "        print(\"錯誤: 網址結構不符合預期 (缺少 'index' 或 '.html')\")\n",
        "        return None\n",
        "    except ValueError:\n",
        "        print(\"錯誤: 提取到的內容無法轉換為數字\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "a3uyvNdzQxoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 指定的起始頁碼\n",
        "START_INDEX = extract_index_split(current_url)\n",
        "# 想要取得的頁數\n",
        "PAGES_TO_FETCH = 10\n",
        "\n",
        "# PTT 基礎網址\n",
        "BASE_URL = \"https://www.ptt.cc/bbs/movie/index\"\n",
        "\n",
        "# 計算迴圈的結束點 (例如: 10808 - 10 + 1 = 10799)\n",
        "# range(start, stop, step) 的 stop 是不包含的，所以我們設為 START_INDEX - PAGES_TO_FETCH\n",
        "stop_index = START_INDEX - PAGES_TO_FETCH\n",
        "\n",
        "print(f\"--- 正在生成從 {START_INDEX} 到 {stop_index + 1} 的 {PAGES_TO_FETCH} 個網址 ---\")\n",
        "\n",
        "# 使用 range 迴圈，從 START_INDEX 遞減到 stop_index\n",
        "for index in range(START_INDEX, stop_index, -1):\n",
        "    # 組合完整的 URL\n",
        "    url = f\"{BASE_URL}{index}.html\"\n",
        "    print(url)\n",
        "    response = requests.get(url, headers=headers, timeout=5)\n",
        "    html_content = response.text\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    article_list = soup.find_all('div', class_='r-ent')\n",
        "    for article in article_list:\n",
        "        # 標題 (Title) 和 連結 (Href) 資訊通常在 div class=\"title\" 內\n",
        "        title_tag = article.find('div', class_='title').find('a')\n",
        "\n",
        "        # 排除被刪除或不可存取(標題為 - )的文章\n",
        "        if title_tag:\n",
        "            title = title_tag.text.strip()\n",
        "            href = \"https://www.ptt.cc\" + title_tag['href']\n",
        "        else:\n",
        "            # 處理被刪除的文章 (通常標題會是 '-')\n",
        "            title = article.find('div', class_='title').text.strip()\n",
        "            href = \"N/A (已刪除或不可存取)\"\n",
        "\n",
        "        # 作者 (Author) 資訊在 div class=\"author\" 內\n",
        "        author = article.find('div', class_='author').text.strip()\n",
        "\n",
        "        # 日期 (Date) 資訊在 div class=\"date\" 內\n",
        "        date = article.find('div', class_='date').text.strip()\n",
        "\n",
        "        # 儲存資料\n",
        "        articles_data.append({\n",
        "            'title': title,\n",
        "            'date': date,\n",
        "            'author': author,\n",
        "            'href': href\n",
        "        })\n",
        "\n",
        "        # 額外要求：如果標題是指定標題，印出其對應的 href\n",
        "        # 假設我們指定要特別關注標題包含 \"新聞\" 的文章\n",
        "        # if '新聞' in title:\n",
        "        #     print(f\"[特別關注] 標題：{title} | 連結：{href}\")\n",
        "    # 註釋: 在實際的爬蟲程式中，您會將這個 url 傳遞給 requests.get() 函式來獲取內容\n",
        "    # 例如: html_content = requests.get(url, headers=headers, cookies=cookies).text"
      ],
      "metadata": {
        "id": "W9wtW_iWqTCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_data"
      ],
      "metadata": {
        "id": "IBGIaxV7rT5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = articles_data[1]['title']"
      ],
      "metadata": {
        "id": "1vfOZfJqZYTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "\n",
        "# 待分詞的中文句子\n",
        "print(f\"原始句子: {sentence}\\n\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# --- 1. 精確模式 (Default Mode) ---\n",
        "# 這是最常用的模式，嘗試將句子最精確地切開，適合文本分析。\n",
        "print(\"模式一：精確模式 (jieba.cut)\")\n",
        "# jieba.cut 返回的是一個迭代器 (iterator)\n",
        "seg_list_precise = jieba.cut(sentence, cut_all=False)\n",
        "# 使用 '/ ' 將結果串接起來，方便輸出\n",
        "print(f\"分詞結果: {'/ '.join(seg_list_precise)}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# --- 2. 全模式 (Full Mode) ---\n",
        "# 會掃描出句子中所有可能的詞語，速度最快，但結果可能有大量重疊。\n",
        "print(\"模式二：全模式 (jieba.cut(..., cut_all=True))\")\n",
        "seg_list_all = jieba.cut(sentence, cut_all=True)\n",
        "print(f\"分詞結果: {'/ '.join(seg_list_all)}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# --- 3. 搜尋引擎模式 (Search Engine Mode) ---\n",
        "# 在精確模式的基礎上，對長詞再次進行細分，適合用於搜尋引擎建立索引。\n",
        "print(\"模式三：搜尋引擎模式 (jieba.cut_for_search)\")\n",
        "seg_list_search = jieba.cut_for_search(sentence)\n",
        "print(f\"分詞結果: {'/ '.join(seg_list_search)}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# --- 4. 詞性標註 (Optional: Add Part-of-Speech Tagging) ---\n",
        "# jieba.posseg 可以在分詞的同時標註詞性 (例如 n: 名詞, v: 動詞)\n",
        "import jieba.posseg as pseg\n",
        "print(\"模式四：分詞與詞性標註 (jieba.posseg)\")\n",
        "words = pseg.cut(sentence)\n",
        "result = []\n",
        "for word, flag in words:\n",
        "    result.append(f\"{word}/{flag}\")\n",
        "\n",
        "print(f\"分詞結果: {' '.join(result)}\")\n",
        "print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "8Lip1mACZbzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 1. 初始化一個 Counter 物件來記錄所有單詞的頻率\n",
        "word_counts = Counter()\n",
        "\n",
        "# 2. 設定一個範圍，從索引 1 到 10 (包含)\n",
        "# 在 Python 的 range 中，range(start, stop) 是從 start 到 stop-1\n",
        "for i in range(1, 11):\n",
        "    # 確保索引 i 存在，避免錯誤\n",
        "    if i < len(articles_data) and 'title' in articles_data[i]:\n",
        "        # 取出標題內容\n",
        "        title_text = articles_data[i]['title']\n",
        "\n",
        "        # --- 文本清理步驟 (重要) ---\n",
        "        # 清除標點符號、空格、換行符等非中文字符\n",
        "        # 使用正則表達式，保留中文、英文字母、數字\n",
        "        cleaned_text = re.sub(r'[^\\w\\s]', '', title_text) # 移除大部分標點符號\n",
        "\n",
        "        # 進行結巴分詞 (使用精確模式 jieba.cut)\n",
        "        # cut() 返回的是一個 generator，通常會轉成 list\n",
        "        # 或直接在迴圈中使用，這裡我們直接用來更新 Counter\n",
        "        words = jieba.cut(cleaned_text, cut_all=False)\n",
        "\n",
        "        # 將分詞結果更新到 word_counts\n",
        "        # Counter 的 update() 方法可以直接接收一個可疊代對象（如 words）\n",
        "        word_counts.update(words)\n",
        "\n",
        "# 3. 清理分詞結果，移除空格、單個字母等常見雜訊\n",
        "# 建立一個新的 Counter，只包含長度大於 1 的詞，或您認為有意義的詞\n",
        "final_word_counts = Counter()\n",
        "for word, count in word_counts.items():\n",
        "    # 移除空字符串、空格、換行符\n",
        "    if word.strip() and len(word.strip()) > 1:\n",
        "        final_word_counts[word] = count\n",
        "\n",
        "# 4. 輸出詞頻結果 (例如，前 10 個高頻詞)\n",
        "print(\"--- 詞頻統計結果 (前 10 名) ---\")\n",
        "for word, count in final_word_counts.most_common(10):\n",
        "    print(f\"'{word}': {count} 次\")"
      ],
      "metadata": {
        "id": "MbvxTVkPZe1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "\n",
        "# 設置一個簡單的停用詞列表（Stop Words）\n",
        "# 這些詞通常頻率很高，但對文章主題貢獻小\n",
        "stopwords = set(['的', '了', '是', '在', '我', '你', '他', '她', '之', '一個', '和', '討論', '分享'])"
      ],
      "metadata": {
        "id": "ota8nHkeZodB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_list = []\n",
        "\n",
        "# 從索引 1 到 10 (包含)\n",
        "for i in range(1, 11):\n",
        "    if i < len(articles_data) and 'title' in articles_data[i]:\n",
        "        title_text = articles_data[i]['title']\n",
        "\n",
        "        # 文本清理：移除標點符號和非詞語字符\n",
        "        cleaned_text = re.sub(r'[^\\w\\s]', '', title_text)\n",
        "\n",
        "        # 進行結巴分詞\n",
        "        # lcut() 直接返回一個列表\n",
        "        words = jieba.lcut(cleaned_text, cut_all=False)\n",
        "\n",
        "        # 過濾停用詞和單個空白詞，並用空格重新連接成一個字符串，以便 TfidfVectorizer 處理\n",
        "        filtered_words = [\n",
        "            word.strip()\n",
        "            for word in words\n",
        "            if word.strip() and len(word.strip()) > 1 and word.strip() not in stopwords\n",
        "        ]\n",
        "\n",
        "        # TfidfVectorizer 需要的是字串形式的文檔\n",
        "        document = \" \".join(filtered_words)\n",
        "        document_list.append(document)\n",
        "\n",
        "# document_list 現在是一個列表，每個元素是經過處理的標題字串\n",
        "# print(document_list)"
      ],
      "metadata": {
        "id": "TmNXPjV0ZslM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 初始化 TfidfVectorizer\n",
        "# TfidfVectorizer 會處理：\n",
        "#    a. 將文檔轉換為詞頻矩陣 (CountVectorizer 的工作)\n",
        "#    b. 計算 TF-IDF 權重 (TfidfTransformer 的工作)\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# 2. 進行擬合和轉換 (Fit and Transform)\n",
        "# tfidf_matrix 是一個稀疏矩陣 (sparse matrix)，包含所有文檔的 TF-IDF 權重\n",
        "tfidf_matrix = vectorizer.fit_transform(document_list)\n",
        "\n",
        "# 3. 獲取所有詞彙 (特徵名稱)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# 4. 將稀疏矩陣轉換為 NumPy 陣列，方便查看權重\n",
        "tfidf_array = tfidf_matrix.toarray()"
      ],
      "metadata": {
        "id": "SCUXqQAKZtcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 創建一個字典來存儲每個詞彙及其在所有文檔中的 TF-IDF 平均權重\n",
        "avg_tfidf_scores = defaultdict(float)\n",
        "\n",
        "# 遍歷所有文檔的權重\n",
        "for doc_weights in tfidf_array:\n",
        "    # 遍歷單篇文檔中的所有詞彙及其權重\n",
        "    for i, weight in enumerate(doc_weights):\n",
        "        word = feature_names[i]\n",
        "        avg_tfidf_scores[word] += weight\n",
        "\n",
        "# 計算平均值\n",
        "num_documents = len(document_list)\n",
        "for word in avg_tfidf_scores:\n",
        "    avg_tfidf_scores[word] /= num_documents\n",
        "\n",
        "# 按權重降序排列\n",
        "sorted_avg_tfidf = sorted(avg_tfidf_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "print(\"--- 整個文檔集合中詞彙的 TF-IDF 平均權重 (前 10 名) ---\")\n",
        "for word, avg_weight in sorted_avg_tfidf[:10]:\n",
        "    print(f\"'{word}': {avg_weight:.4f}\")"
      ],
      "metadata": {
        "id": "-phQ9NdnZwYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============\n",
        "# PTT 電影版爬蟲\n",
        "# ==============\n",
        "PTT_MOVIE_INDEX = \"https://www.ptt.cc/bbs/movie/index.html\"\n",
        "PTT_COOKIES = {\"over18\": \"1\"}\n",
        "\n",
        "def _get_soup(url):\n",
        "    r = requests.get(url, timeout=15, headers={\"User-Agent\":\"Mozilla/5.0\"}, cookies=PTT_COOKIES)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "def _get_prev_index_url(soup):\n",
        "    btns = soup.select(\"div.btn-group-paging a.btn.wide\")\n",
        "    # 頁面通常有三個：最舊 ← 上一頁 ← 最前頁；取「上一頁」\n",
        "    for a in btns:\n",
        "        if \"上頁\" in a.get_text(strip=True):\n",
        "            href = a.get(\"href\")\n",
        "            if href:\n",
        "                return \"https://www.ptt.cc\" + href\n",
        "    return None\n",
        "\n",
        "def _parse_nrec(nrec_span):\n",
        "    # 可能是數字或 \"爆\"/\"X1\" 等\n",
        "    if not nrec_span:\n",
        "        return 0\n",
        "    txt = nrec_span.get_text(strip=True)\n",
        "    if txt == \"爆\":\n",
        "        return 100\n",
        "    if txt.startswith(\"X\"):\n",
        "        try:\n",
        "            return -int(txt[1:])\n",
        "        except:\n",
        "            return -10\n",
        "    try:\n",
        "        return int(txt)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def _extract_post_list(soup):\n",
        "    posts = []\n",
        "    for r in soup.select(\"div.r-ent\"):\n",
        "        a = r.select_one(\"div.title a\")\n",
        "        if not a:\n",
        "            continue\n",
        "        title = a.get_text(strip=True)\n",
        "        url = \"https://www.ptt.cc\" + a.get(\"href\")\n",
        "        author = r.select_one(\"div.author\").get_text(strip=True)\n",
        "        date = r.select_one(\"div.date\").get_text(strip=True)\n",
        "        nrec = _parse_nrec(r.select_one(\"div.nrec span\"))\n",
        "        posts.append({\n",
        "            \"title\": title, \"url\": url, \"author\": author, \"date\": date, \"nrec\": nrec\n",
        "        })\n",
        "    return posts\n",
        "\n",
        "def _clean_ptt_content(soup):\n",
        "    # 移除推文區\n",
        "    for p in soup.select(\"div.push\"):\n",
        "        p.decompose()\n",
        "    main = soup.select_one(\"#main-content\")\n",
        "    if not main:\n",
        "        return \"\", \"\"\n",
        "    # 移除看板的 meta 資訊行（作者/標題/時間）\n",
        "    metas = main.select(\"div.article-metaline, div.article-metaline-right\")\n",
        "    for m in metas:\n",
        "        m.decompose()\n",
        "    # 取出內文與簽名檔前切割\n",
        "    text = main.get_text(\"\\n\", strip=True)\n",
        "    if \"--\" in text:\n",
        "        text = text.split(\"--\")[0].strip()\n",
        "    # 擷取標題（有些文章標題可再補救）\n",
        "    title_tag = soup.select_one(\"span.article-meta-value\")\n",
        "    meta_title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "    return text, meta_title\n",
        "\n",
        "def crawl_ptt_movie(index_pages=3, min_push=0, keyword=\"\"):\n",
        "    \"\"\"從最新 index.html 往前翻 index_pages 頁，抓滿足條件的文章\"\"\"\n",
        "    global ptt_posts_df\n",
        "    url = PTT_MOVIE_INDEX\n",
        "    all_rows = []\n",
        "    seen_urls = set(ptt_posts_df[\"url\"].tolist()) if not ptt_posts_df.empty else set()\n",
        "\n",
        "    for _ in range(int(index_pages)):\n",
        "        soup = _get_soup(url)\n",
        "        posts = _extract_post_list(soup)\n",
        "        # 篩選\n",
        "        for p in posts:\n",
        "            if p[\"nrec\"] < int(min_push):\n",
        "                continue\n",
        "            if keyword and (keyword not in p[\"title\"]):\n",
        "                continue\n",
        "            # 去重（避免反覆抓同一篇）\n",
        "            if p[\"url\"] in seen_urls:\n",
        "                continue\n",
        "\n",
        "            # 抓正文\n",
        "            try:\n",
        "                art_soup = _get_soup(p[\"url\"])\n",
        "                content, meta_title = _clean_ptt_content(art_soup)\n",
        "            except Exception as e:\n",
        "                content, meta_title = \"\", \"\"\n",
        "            final_title = p[\"title\"] if p[\"title\"] else (meta_title or \"（無標題）\")\n",
        "\n",
        "            all_rows.append({\n",
        "                \"post_id\": str(uuid.uuid4())[:8],\n",
        "                \"title\": final_title[:200],\n",
        "                \"url\": p[\"url\"],\n",
        "                \"date\": p[\"date\"],\n",
        "                \"author\": p[\"author\"],\n",
        "                \"nrec\": str(p[\"nrec\"]),\n",
        "                \"created_at\": tznow().isoformat(),\n",
        "                \"fetched_at\": tznow().isoformat(),\n",
        "                \"content\": content\n",
        "            })\n",
        "\n",
        "        # 換上一頁\n",
        "        prev = _get_prev_index_url(soup)\n",
        "        if not prev:\n",
        "            break\n",
        "        url = prev\n",
        "\n",
        "    if all_rows:\n",
        "        new_df = pd.DataFrame(all_rows, columns=PTT_HEADER)\n",
        "        ptt_posts_df = pd.concat([ptt_posts_df, new_df], ignore_index=True)\n",
        "        write_df(ws_ptt_posts, ptt_posts_df, PTT_HEADER)\n",
        "        return f\"✅ 取得 {len(all_rows)} 篇文章（已寫入 Sheet）\", ptt_posts_df\n",
        "    else:\n",
        "        return \"ℹ️ 沒有新文章符合條件（或內容已在 Sheet）\", ptt_posts_df\n",
        "\n",
        "\n",
        "# ==============\n",
        "# 文本分析（jieba + TF/IDF + bigram）\n",
        "# ==============\n",
        "import re\n",
        "try:\n",
        "    import jieba\n",
        "except:\n",
        "    jieba = None\n",
        "\n",
        "def _tokenize_zh(text):\n",
        "    text = re.sub(r\"[^\\u4e00-\\u9fffA-Za-z0-9]+\", \" \", text)\n",
        "    if not jieba:\n",
        "        # 後備：用空白切（較差，但避免無法執行）\n",
        "        return [t for t in text.split() if len(t) > 1]\n",
        "    return [w.strip() for w in jieba.lcut(text) if len(w.strip()) > 1]\n",
        "\n",
        "def analyze_ptt_texts(topk=50, min_df=2):\n",
        "    global ptt_posts_df, terms_df\n",
        "    if ptt_posts_df.empty:\n",
        "        return \"📭 尚無已抓取的文章，請先在『PTT 電影爬蟲』分頁取得文章。\", terms_df, \"\"\n",
        "\n",
        "    docs = []\n",
        "    for _, r in ptt_posts_df.iterrows():\n",
        "        # 將標題與內文拼起來提高關鍵詞可見度\n",
        "        docs.append((r[\"title\"] or \"\") + \"\\n\" + (r[\"content\"] or \"\"))\n",
        "\n",
        "    # 詞頻\n",
        "    from collections import Counter, defaultdict\n",
        "    freq = Counter()\n",
        "    df_cnt = defaultdict(int)\n",
        "\n",
        "    token_docs = []\n",
        "    for doc in docs:\n",
        "        toks = _tokenize_zh(doc)\n",
        "        token_docs.append(toks)\n",
        "        freq.update(toks)\n",
        "        for t in set(toks):\n",
        "            df_cnt[t] += 1\n",
        "\n",
        "    # TF-IDF（平均值）\n",
        "    try:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        vec = TfidfVectorizer(tokenizer=_tokenize_zh, lowercase=False, min_df=min_df)\n",
        "        X = vec.fit_transform(docs)\n",
        "        terms = vec.get_feature_names_out()\n",
        "        tfidf_mean = X.mean(axis=0).A1\n",
        "        tfidf_map = dict(zip(terms, tfidf_mean))\n",
        "    except Exception:\n",
        "        tfidf_map = {t: 0.0 for t in freq.keys()}\n",
        "\n",
        "    # Bigram（粗略）\n",
        "    from itertools import tee\n",
        "    def pairwise(iterable):\n",
        "        a, b = tee(iterable)\n",
        "        next(b, None)\n",
        "        return zip(a, b)\n",
        "    bigram_freq = Counter()\n",
        "    for toks in token_docs:\n",
        "        bigram_freq.update([\" \".join(bg) for bg in pairwise(toks)])\n",
        "\n",
        "    # 取 TopK 關鍵詞（綜合：先按 tfidf，再用 freq 當次排序）\n",
        "    candidates = list(freq.keys())\n",
        "    candidates.sort(key=lambda t: (round(tfidf_map.get(t,0.0), 6), freq[t]), reverse=True)\n",
        "    top_terms = candidates[:int(topk)]\n",
        "\n",
        "    # 範例句（簡單從任一篇取 1 個樣例片段）\n",
        "    examples = {}\n",
        "    for term in top_terms:\n",
        "        ex = \"\"\n",
        "        for doc in docs:\n",
        "            if term in doc:\n",
        "                # 取出包含該詞的片段（±15字）\n",
        "                i = doc.find(term)\n",
        "                s = max(0, i-15)\n",
        "                e = min(len(doc), i+len(term)+15)\n",
        "                ex = doc[s:e].replace(\"\\n\",\" \")\n",
        "                break\n",
        "        examples[term] = ex\n",
        "\n",
        "    rows = []\n",
        "    for t in top_terms:\n",
        "        rows.append({\n",
        "            \"term\": t,\n",
        "            \"freq\": str(freq[t]),\n",
        "            \"df_count\": str(df_cnt[t]),\n",
        "            \"tfidf_mean\": f\"{tfidf_map.get(t,0.0):.6f}\",\n",
        "            \"examples\": examples.get(t, \"\")\n",
        "        })\n",
        "    terms_df = pd.DataFrame(rows, columns=TERMS_HEADER)\n",
        "    write_df(ws_ptt_terms, terms_df, TERMS_HEADER)\n",
        "\n",
        "    # 產生 Markdown 摘要\n",
        "    md_lines = []\n",
        "    md_lines.append(f\"### 關鍵詞 Top {len(top_terms)}（依 TF-IDF 平均值優先，次序再以詞頻）\")\n",
        "    for i, t in enumerate(top_terms, 1):\n",
        "        md_lines.append(f\"{i}. **{t}** — tfidf≈{float(tfidf_map.get(t,0.0)):.4f}；freq={freq[t]}；df={df_cnt[t]}\")\n",
        "    md_lines.append(\"\\n### 常見雙詞搭配（前 20）\")\n",
        "    for i, (bg, c) in enumerate(bigram_freq.most_common(20), 1):\n",
        "        md_lines.append(f\"{i}. {bg} — {c}\")\n",
        "\n",
        "    return f\"✅ 已完成文本分析，共 {len(docs)} 篇文章；關鍵詞已寫入 Sheet。\", terms_df, \"\\n\".join(md_lines)"
      ],
      "metadata": {
        "id": "XKJcaSD7UwoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_df(ws, header):\n",
        "    df = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(columns=header)\n",
        "    df = df.fillna(\"\")\n",
        "    # 保證欄位齊全\n",
        "    for c in header:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\"\n",
        "    # 型別微調\n",
        "    if \"est_min\" in df.columns:\n",
        "        df[\"est_min\"] = pd.to_numeric(df[\"est_min\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    if \"actual_min\" in df.columns:\n",
        "        df[\"actual_min\"] = pd.to_numeric(df[\"actual_min\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    if \"pomodoros\" in df.columns:\n",
        "        df[\"pomodoros\"] = pd.to_numeric(df[\"pomodoros\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    return df[header]\n",
        "\n",
        "# 爬蟲：擷取文字或連結並可加入任務\n",
        "# =========================\n",
        "def crawl(url, selector, mode, limit):\n",
        "    try:\n",
        "        resp = requests.get(url, timeout=15, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
        "        resp.raise_for_status()\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(columns=CLIPS_HEADER), f\"⚠️ 請求失敗：{e}\"\n",
        "\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    nodes = soup.select(selector)\n",
        "    rows = []\n",
        "    for i, n in enumerate(nodes[:int(limit) if limit else 20]):\n",
        "        text = n.get_text(strip=True) if mode in (\"text\",\"both\") else \"\"\n",
        "        href = n.get(\"href\") if mode in (\"href\",\"both\") else \"\"\n",
        "        # 相對連結處理\n",
        "        if href and href.startswith(\"/\"):\n",
        "            from urllib.parse import urljoin\n",
        "            href = urljoin(url, href)\n",
        "        rows.append({\n",
        "            \"clip_id\": str(uuid.uuid4())[:8],\n",
        "            \"url\": url,\n",
        "            \"selector\": selector,\n",
        "            \"text\": text,\n",
        "            \"href\": href,\n",
        "            \"created_at\": tznow().isoformat(),\n",
        "            \"added_to_task\": \"\"\n",
        "        })\n",
        "    df = pd.DataFrame(rows, columns=CLIPS_HEADER)\n",
        "    return df, f\"✅ 擷取 {len(df)} 筆\"\n",
        "\n",
        "def add_clips_as_tasks(clip_ids, default_priority, est_min):\n",
        "    global clips_df, tasks_df\n",
        "    if not clip_ids:\n",
        "        return \"⚠️ 請先勾選要加入的爬蟲項目\", clips_df, tasks_df\n",
        "    sel = clips_df[clips_df[\"clip_id\"].isin(clip_ids)]\n",
        "    _now = tznow().isoformat()\n",
        "    new_tasks = []\n",
        "    for _, r in sel.iterrows():\n",
        "        title = r[\"text\"] or r[\"href\"] or \"（未命名）\"\n",
        "        note = f\"來源：{r['url']}\\n選擇器：{r['selector']}\\n連結：{r['href']}\"\n",
        "        new_tasks.append({\n",
        "            \"id\": str(uuid.uuid4())[:8],\n",
        "            \"task\": title[:120],\n",
        "            \"status\": \"todo\",\n",
        "            \"priority\": default_priority or \"M\",\n",
        "            \"est_min\": int(est_min) if est_min else 25,\n",
        "            \"start_time\": \"\",\n",
        "            \"end_time\": \"\",\n",
        "            \"actual_min\": 0,\n",
        "            \"pomodoros\": 0,\n",
        "            \"due_date\": \"\",\n",
        "            \"labels\": \"from:crawler\",\n",
        "            \"notes\": note,\n",
        "            \"created_at\": _now,\n",
        "            \"updated_at\": _now,\n",
        "            \"completed_at\": \"\",\n",
        "            \"planned_for\": \"\"\n",
        "        })\n",
        "    if new_tasks:\n",
        "        tasks_df = pd.concat([tasks_df, pd.DataFrame(new_tasks)], ignore_index=True)\n",
        "        # 標記已加入\n",
        "        clips_df.loc[clips_df[\"clip_id\"].isin(clip_ids), \"added_to_task\"] = \"yes\"\n",
        "        write_df(ws_tasks, tasks_df, TASKS_HEADER)\n",
        "        write_df(ws_clips, clips_df, CLIPS_HEADER)\n",
        "        return f\"✅ 已加入 {len(new_tasks)} 項為任務\", clips_df, tasks_df\n",
        "    return \"⚠️ 無可加入項目\", clips_df, tasks_df\n",
        "\n",
        "\n",
        "def read_ptt_posts_df():\n",
        "    return read_df(ws_ptt_posts, PTT_HEADER).copy()\n",
        "\n",
        "def read_terms_df():\n",
        "    return read_df(ws_ptt_terms, TERMS_HEADER).copy()\n",
        "\n",
        "ptt_posts_df = read_ptt_posts_df()\n",
        "terms_df = read_terms_df()\n"
      ],
      "metadata": {
        "id": "uCOjt-sDVK39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 核心功能函數：Google Sheet 讀寫、爬蟲深度、TF-IDF、GeminiAPI\n",
        "1.新增：資料讀寫與文章內容爬取函數"
      ],
      "metadata": {
        "id": "IgpxkmEpW-ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_df(worksheet, df, header):\n",
        "    \"\"\"將 DataFrame 寫入 Google Sheet，並確保表頭正確\"\"\"\n",
        "    if df.empty:\n",
        "        return 0\n",
        "\n",
        "    # 確保 DataFrame 只有表頭指定的欄位\n",
        "    data_to_write = [header] + df[header].fillna('').astype(str).values.tolist()\n",
        "\n",
        "    # 計算更新範圍\n",
        "    num_rows = len(data_to_write)\n",
        "    num_cols = len(header)\n",
        "    range_label = f'A1:{chr(ord(\"A\") + num_cols - 1)}{num_rows}'\n",
        "\n",
        "    worksheet.clear()\n",
        "    worksheet.update(data_to_write, range_name=range_label)\n",
        "    return len(data_to_write) - 1\n",
        "\n",
        "def read_df(worksheet, header):\n",
        "    \"\"\"從 Google Sheet 讀取資料並轉換成 DataFrame\"\"\"\n",
        "    data = worksheet.get_all_values()\n",
        "    if not data or data[0] != header:\n",
        "        return pd.DataFrame([], columns=header)\n",
        "    # 第一行為表頭，從第二行開始才是資料\n",
        "    return pd.DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "def get_article_content(article_url):\n",
        "    \"\"\"獲取單一 PTT 文章的內容和詳細資訊\"\"\"\n",
        "    if \"N/A\" in article_url:\n",
        "        return \"N/A\", \"N/A\", \"N/A\"\n",
        "    try:\n",
        "        res = requests.get(article_url, headers=headers, timeout=5)\n",
        "        soup = BeautifulSoup(res.text, 'html.parser')\n",
        "        main_content = soup.find(id=\"main-content\")\n",
        "\n",
        "        # 移除文章資訊 (meta lines) 和推文區\n",
        "        for tag in main_content.find_all('div', class_='article-metaline'):\n",
        "            tag.extract()\n",
        "        for tag in main_content.find_all('div', class_='article-metaline-right'):\n",
        "            tag.extract()\n",
        "        for push in main_content.find_all('div', class_='push'):\n",
        "            push.extract()\n",
        "\n",
        "        # 提取剩下的文字內容\n",
        "        content = main_content.text.strip()\n",
        "\n",
        "        # 處理 post_id, created_at (從網址中提取 Unix Timestamp)\n",
        "        post_id_match = re.search(r'M\\.(\\d+)\\.A\\.', article_url)\n",
        "        post_id = post_id_match.group(1) if post_id_match else uuid.uuid4().hex\n",
        "\n",
        "        try:\n",
        "            created_at_ts = int(post_id)\n",
        "            # 將 Unix Timestamp 轉換為 ISO 格式時間\n",
        "            created_at = dt.fromtimestamp(created_at_ts, tz=gettz(TIMEZONE)).isoformat()\n",
        "        except Exception:\n",
        "            created_at = dt.now(gettz(TIMEZONE)).isoformat()\n",
        "\n",
        "        return post_id, content, created_at\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching article {article_url}: {e}\")\n",
        "        return uuid.uuid4().hex, \"N/A\", dt.now(gettz(TIMEZONE)).isoformat()"
      ],
      "metadata": {
        "id": "o8WvPY1oVlHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.新增：TF-IDF 關鍵字分析函數"
      ],
      "metadata": {
        "id": "tD81nQnxXJBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tfidf(posts_df, top_n=20):\n",
        "    \"\"\"\n",
        "    執行中文斷詞和 TF-IDF 計算，找出前 N 熱詞。\n",
        "    \"\"\"\n",
        "    # 確保只分析有內容的貼文\n",
        "    posts_df = posts_df[posts_df['content'].str.strip() != 'N/A'].copy()\n",
        "    if posts_df.empty:\n",
        "        # 如果沒有有效資料，回傳空的 DataFrame\n",
        "        return pd.DataFrame([], columns=TERMS_HEADER)\n",
        "\n",
        "    # 進行中文斷詞：使用 Jieba 預設詞典\n",
        "    # 已移除 jieba.set_dictionary(\"dict.txt.big\") 以避免檔案不存在的錯誤\n",
        "    posts_df['segmented_content'] = posts_df['content'].apply(\n",
        "        lambda x: \" \".join(jieba.cut(x.replace('\\n', ' ').replace('\\r', ''), cut_all=False))\n",
        "    )\n",
        "\n",
        "    # TF-IDF 設定 (移除純數字、英文單詞等)\n",
        "    tfidf = TfidfVectorizer(\n",
        "        stop_words=[],\n",
        "        token_pattern=r\"[\\u4e00-\\u9fa5]{2,}\", # 至少兩個中文字的詞彙\n",
        "        min_df=5, # 至少在 5 篇文章中出現\n",
        "    )\n",
        "\n",
        "    # 過濾掉分詞後為空字串的列，避免 fit_transform 失敗\n",
        "    valid_docs = posts_df[posts_df['segmented_content'].str.strip() != '']['segmented_content']\n",
        "    if valid_docs.empty:\n",
        "        return pd.DataFrame([], columns=TERMS_HEADER)\n",
        "\n",
        "    tfidf_matrix = tfidf.fit_transform(valid_docs)\n",
        "    feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "    # 1. 計算平均 TF-IDF 分數 (作為權重)\n",
        "    avg_tfidf_scores = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
        "    df_results = pd.DataFrame({'term': feature_names, 'tfidf_mean': avg_tfidf_scores})\n",
        "\n",
        "    # 2. 計算文件頻率 (df_count)\n",
        "    df_count_map = defaultdict(int)\n",
        "    for doc in valid_docs:\n",
        "        # 只計算在此次 TF-IDF 分析中被採用的詞彙\n",
        "        unique_words = set(re.findall(tfidf.token_pattern, doc))\n",
        "        for word in unique_words:\n",
        "            df_count_map[word] += 1\n",
        "\n",
        "    df_count = pd.DataFrame(df_count_map.items(), columns=['term', 'df_count'])\n",
        "\n",
        "    # 3. 計算總詞頻 (freq)\n",
        "    word_counts = defaultdict(int)\n",
        "    for doc in valid_docs:\n",
        "        for word in re.findall(tfidf.token_pattern, doc):\n",
        "            word_counts[word] += 1\n",
        "\n",
        "    df_freq = pd.DataFrame(word_counts.items(), columns=['term', 'freq'])\n",
        "\n",
        "    # 合併結果\n",
        "    df_combined = pd.merge(df_results, df_count, on='term', how='inner')\n",
        "    df_combined = pd.merge(df_combined, df_freq, on='term', how='inner')\n",
        "\n",
        "    # 找出 top N 熱詞\n",
        "    top_terms = df_combined.sort_values(by='tfidf_mean', ascending=False).head(top_n)\n",
        "\n",
        "    # 提取例子 (從文章標題)\n",
        "    example_map = {}\n",
        "    # 使用 posts_df，因為它包含 'title' 欄位\n",
        "    posts_for_examples = posts_df[posts_df['segmented_content'].str.strip() != '']\n",
        "\n",
        "    for term in top_terms['term']:\n",
        "        # 找出包含該詞彙的文章標題 (最多 3 個)\n",
        "        examples = posts_for_examples[posts_for_examples['segmented_content'].str.contains(r'\\b' + re.escape(term) + r'\\b', na=False)]['title'].head(3).tolist()\n",
        "        example_map[term] = \" / \".join(examples)\n",
        "\n",
        "    top_terms['examples'] = top_terms['term'].map(example_map)\n",
        "\n",
        "    return top_terms[TERMS_HEADER] # 按照表頭順序回傳"
      ],
      "metadata": {
        "id": "CoIvLsMVXQaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.新增：Gemini API 洞察生成函數"
      ],
      "metadata": {
        "id": "lFRHIRm9XUbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "\n",
        "def generate_insights(top_terms_df, model):\n",
        "    \"\"\"\n",
        "    串接 Gemini API 根據熱詞結果生成洞察摘要和結論，並增加健壯性檢查。\n",
        "    \"\"\"\n",
        "    if top_terms_df.empty:\n",
        "        return \"⚠️ 資料不足：TF-IDF 分析結果為空，無法生成洞察摘要和結論。\"\n",
        "\n",
        "    # 準備提示詞\n",
        "    top_terms_list = top_terms_df.to_dict('records')\n",
        "    # 這裡只取需要的欄位來組裝提示詞，避免傳遞不必要的資訊\n",
        "    formatted_terms = \"\\n\".join([\n",
        "        f\"- {item['term']} (平均 TF-IDF: {float(item['tfidf_mean']):.4f}, 總頻率: {item['freq']})\"\n",
        "        for item in top_terms_list\n",
        "    ])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    你是一位專業的市場分析師。請根據以下 PTT Movie 板的熱詞分析結果，為近期討論產出專業的洞察摘要與結論。\n",
        "\n",
        "    熱詞分析結果 (Top {len(top_terms_list)}):\n",
        "    {formatted_terms}\n",
        "\n",
        "    請嚴格以繁體中文完成以下要求，並直接輸出結果，不包含任何額外解釋：\n",
        "    1.  **5 句洞察摘要**：總結目前的討論趨勢、熱門話題或用戶情緒，每句獨立成段，句首加上「洞察X：」。\n",
        "    2.  **一段 120 字結論**：用一段話總結整體情況，並提供一個未來觀察建議。結論字數約在 100-140 字之間。\n",
        "\n",
        "    輸出格式範例:\n",
        "    洞察1：...\n",
        "    洞察2：...\n",
        "    洞察3：...\n",
        "    洞察4：...\n",
        "    洞察5：...\n",
        "\n",
        "    結論：... (約120字)\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "\n",
        "        # --- 關鍵修正點：檢查回覆是否被安全過濾阻擋 ---\n",
        "        # 如果 candidates 列表為空，通常表示回應被阻擋\n",
        "        if not response.candidates:\n",
        "             # 嘗試獲取 safety rating\n",
        "            safety_reason = \"未知原因\"\n",
        "            if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                safety_reason = f\"Block Reason: {response.prompt_feedback.block_reason.name}\"\n",
        "\n",
        "            return f\"⚠️ Gemini API 呼叫失敗：模型回覆因內容政策被阻擋。{safety_reason}。請檢查您的熱詞內容。\"\n",
        "\n",
        "        generated_text = response.text.strip()\n",
        "\n",
        "        if not generated_text:\n",
        "            return \"⚠️ Gemini API 呼叫失敗：模型回覆內容為空。請確保提示詞內容足夠豐富。\"\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        # 捕獲網路錯誤、API 金鑰錯誤或其他未預期錯誤\n",
        "        return f\"⚠️ Gemini API 呼叫失敗 (未預期錯誤): {type(e).__name__}: {e}\""
      ],
      "metadata": {
        "id": "ZwtRjxY2XYZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 整合為自動化流程（Gradio 介面）\n",
        "將爬蟲、分析、生成結果的功能整合到一個主要的 run_analysis_pipeline 函數中。"
      ],
      "metadata": {
        "id": "sl1V8WrSXmPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crawl_and_save_data(start_index, pages_to_fetch, ws_posts):\n",
        "    \"\"\"\n",
        "    從 PTT 爬取多頁文章標題，並深入爬取文章內容後寫入 Google Sheet。\n",
        "    \"\"\"\n",
        "    BASE_URL = \"https://www.ptt.cc/bbs/movie/index\"\n",
        "    articles_data = []\n",
        "\n",
        "    stop_index = start_index - pages_to_fetch\n",
        "    fetched_at = dt.now(gettz(TIMEZONE)).isoformat()\n",
        "\n",
        "    for index in range(start_index, stop_index, -1):\n",
        "        url = f\"{BASE_URL}{index}.html\"\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=5)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            article_list = soup.find_all('div', class_='r-ent')\n",
        "\n",
        "            for article in article_list:\n",
        "                title_tag = article.find('div', class_='title').find('a')\n",
        "\n",
        "                if title_tag:\n",
        "                    title = title_tag.text.strip()\n",
        "                    href = \"https://www.ptt.cc\" + title_tag['href']\n",
        "                else:\n",
        "                    title = article.find('div', class_='title').text.strip()\n",
        "                    href = \"N/A (已刪除或不可存取)\"\n",
        "\n",
        "                author = article.find('div', class_='author').text.strip()\n",
        "                date = article.find('div', class_='date').text.strip()\n",
        "\n",
        "                # 深度爬取文章內容\n",
        "                post_id, content, created_at = get_article_content(href)\n",
        "\n",
        "                articles_data.append({\n",
        "                    'post_id': post_id,\n",
        "                    'title': title,\n",
        "                    'url': href,\n",
        "                    'date': date,\n",
        "                    'author': author,\n",
        "                    'nrec': article.find('div', class_='nrec').text.strip(), # 補上熱度\n",
        "                    'created_at': created_at,\n",
        "                    'fetched_at': fetched_at,\n",
        "                    'content': content\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching page {url}: {e}\")\n",
        "            continue\n",
        "\n",
        "    df_posts = pd.DataFrame(articles_data)\n",
        "    posts_written = write_df(ws_posts, df_posts, PTT_HEADER)\n",
        "    return df_posts, posts_written\n",
        "\n",
        "def run_analysis_pipeline(pages_to_fetch):\n",
        "    \"\"\"\n",
        "    自動化流程主函數：爬蟲 -> Sheet -> TF-IDF -> Gemini -> 顯示結果\n",
        "    \"\"\"\n",
        "    # 確保 Google Sheet 連線和工作表存在\n",
        "    try:\n",
        "        sh = gc.open(WORKSHEET_NAME)\n",
        "        ws_ptt_posts = sh.worksheet(\"ptt_movie_posts\")\n",
        "        ws_ptt_terms = sh.worksheet(\"ptt_movie_terms\")\n",
        "    except Exception:\n",
        "        return \"⚠️ 錯誤：請確保 Google Sheets 認證已完成，且工作表名稱正確。\", \"\", \"\", \"\"\n",
        "\n",
        "    # 1. 爬蟲結果 → 寫入 Google Sheet\n",
        "    output_message = \"--- 開始執行自動化流程 ---\\n\"\n",
        "\n",
        "    # 獲取起始頁碼 (使用您原有的邏輯)\n",
        "    responseIndex = requests.get(\"https://www.ptt.cc/bbs/movie/index.html\", headers=headers, timeout=5)\n",
        "    soupIndex = BeautifulSoup(responseIndex.text, 'html.parser')\n",
        "    current_url = get_previous_page_url(soupIndex)\n",
        "    start_index = extract_index_split(current_url)\n",
        "\n",
        "    df_posts, posts_written = crawl_and_save_data(start_index, pages_to_fetch, ws_ptt_posts)\n",
        "    output_message += f\"✅ 1. 爬蟲完成：成功爬取 {posts_written} 篇文章，已寫入「ptt_movie_posts」工作表。\\n\"\n",
        "\n",
        "    # 2. 從 Sheet 讀取資料 → 詞頻與關鍵字統計（TF-IDF）\n",
        "    # 這裡直接使用剛爬取的 df_posts 進行分析，避免再次讀取 Sheet\n",
        "    top_n = 20 # 預設輸出前 20 熱詞\n",
        "    df_terms = calculate_tfidf(df_posts, top_n=top_n)\n",
        "\n",
        "    output_message += f\"✅ 2. TF-IDF 分析完成：已從 {len(df_posts)} 篇貼文計算出 Top {top_n} 熱詞。\\n\"\n",
        "\n",
        "    # 3. 輸出前 N 熱詞 → 回寫到統計表\n",
        "    terms_written = write_df(ws_ptt_terms, df_terms, TERMS_HEADER)\n",
        "    output_message += f\"✅ 3. 熱詞回寫完成：已將 {terms_written} 條熱詞寫入「ptt_movie_terms」工作表。\\n\"\n",
        "\n",
        "    # 4. 串接 Gemini API 生成洞察摘要與結論\n",
        "    insight_output = generate_insights(df_terms, model)\n",
        "    output_message += \"✅ 4. Gemini API 洞察生成完成。\\n\"\n",
        "    output_message += \"---------------------------\\n\"\n",
        "    output_message += \"✨ 洞察摘要與結論已準備好。\"\n",
        "\n",
        "    # 整理結果輸出\n",
        "    # 熱詞結果轉為 Markdown 表格\n",
        "    terms_markdown = df_terms[['term', 'freq', 'tfidf_mean', 'examples']].to_markdown(index=False, floatfmt=\".4f\")\n",
        "\n",
        "    return output_message, insight_output, terms_markdown, SHEET_URL.replace(\"/edit?usp=sharing\", \"/edit\")\n",
        "\n",
        "# 假設您原有的輔助函數 (get_previous_page_url, extract_index_split) 已保留\n",
        "def get_previous_page_url(soup):\n",
        "    paging_div = soup.find('div', class_='btn-group btn-group-paging')\n",
        "    if paging_div:\n",
        "        prev_button = paging_div.find_all('a')[1]\n",
        "        if 'href' in prev_button.attrs:\n",
        "            return \"https://www.ptt.cc\" + prev_button['href']\n",
        "    return None\n",
        "\n",
        "def extract_index_split(url):\n",
        "    try:\n",
        "        index_str = url.split('index')[1].split('.html')[0]\n",
        "        return int(index_str)\n",
        "    except Exception:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "Y3GoiI14Xmo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 整合成 Gradio 介面\n",
        "with gr.Blocks(title=\"文字資料分析自動化流程 (PTT Movie)\") as demo:\n",
        "    gr.Markdown(\"## 🎬 PTT Movie 文字資料自動化分析器\")\n",
        "    gr.Markdown(\"可一鍵執行：**爬蟲** (近 N 頁) → **Google Sheet 寫入** → **TF-IDF 分析** → **熱詞回寫** → **Gemini 洞察摘要/結論生成**。\")\n",
        "\n",
        "    with gr.Row():\n",
        "        pages_input = gr.Slider(\n",
        "            minimum=1,\n",
        "            maximum=50,\n",
        "            step=1,\n",
        "            value=10,\n",
        "            label=\"欲爬取的頁數 (每頁約 20 篇)\"\n",
        "        )\n",
        "\n",
        "        run_button = gr.Button(\"🚀 一鍵執行自動化分析\", variant=\"primary\")\n",
        "\n",
        "    status_output = gr.Textbox(label=\"流程狀態與訊息\", lines=4)\n",
        "    sheet_link_output = gr.HTML(label=\"Google Sheet 連結\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 💎 Gemini AI 洞察摘要與結論\")\n",
        "            ai_output = gr.Textbox(label=\"分析結果\", lines=15, interactive=False)\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 🔥 TF-IDF Top 熱詞統計結果\")\n",
        "            terms_table = gr.Markdown(label=\"熱詞結果 (已回寫至 Google Sheet)\", value=\"等待執行...\")\n",
        "\n",
        "\n",
        "    def pipeline_wrapper(pages):\n",
        "        output, insights, terms_md, sheet_url = run_analysis_pipeline(pages)\n",
        "        link_html = f'<a href=\"{sheet_url}\" target=\"_blank\" style=\"color: blue; text-decoration: underline;\">🔗 點此開啟 Google Sheet 查看原始與統計資料</a>'\n",
        "        return output, insights, terms_md, link_html\n",
        "\n",
        "    run_button.click(\n",
        "        fn=pipeline_wrapper,\n",
        "        inputs=[pages_input],\n",
        "        outputs=[status_output, ai_output, terms_table, sheet_link_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KrjIN4hdXxS-",
        "outputId": "7a90648c-0bfa-4a73-b536-d68aaae9a9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9ff16c3548a55a29f7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://9ff16c3548a55a29f7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2116, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1623, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 976, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 915, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-430219034.py\", line 31, in pipeline_wrapper\n",
            "    output, insights, terms_md, sheet_url = run_analysis_pipeline(pages)\n",
            "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2552998066.py\", line 80, in run_analysis_pipeline\n",
            "    df_terms = calculate_tfidf(df_posts, top_n=top_n)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4239570235.py\", line 11, in calculate_tfidf\n",
            "    jieba.set_dictionary(\"dict.txt.big\") # 假設您有大詞典\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jieba/__init__.py\", line 513, in set_dictionary\n",
            "    raise Exception(\"jieba: file does not exist: \" + abs_path)\n",
            "Exception: jieba: file does not exist: /content/dict.txt.big\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2116, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1623, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 976, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 915, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-430219034.py\", line 31, in pipeline_wrapper\n",
            "    output, insights, terms_md, sheet_url = run_analysis_pipeline(pages)\n",
            "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2552998066.py\", line 80, in run_analysis_pipeline\n",
            "    df_terms = calculate_tfidf(df_posts, top_n=top_n)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4239570235.py\", line 11, in calculate_tfidf\n",
            "    jieba.set_dictionary(\"dict.txt.big\") # 假設您有大詞典\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jieba/__init__.py\", line 513, in set_dictionary\n",
            "    raise Exception(\"jieba: file does not exist: \" + abs_path)\n",
            "Exception: jieba: file does not exist: /content/dict.txt.big\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2116, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1623, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 976, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 915, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-430219034.py\", line 31, in pipeline_wrapper\n",
            "    output, insights, terms_md, sheet_url = run_analysis_pipeline(pages)\n",
            "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2552998066.py\", line 80, in run_analysis_pipeline\n",
            "    df_terms = calculate_tfidf(df_posts, top_n=top_n)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4239570235.py\", line 11, in calculate_tfidf\n",
            "    jieba.set_dictionary(\"dict.txt.big\") # 假設您有大詞典\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jieba/__init__.py\", line 513, in set_dictionary\n",
            "    raise Exception(\"jieba: file does not exist: \" + abs_path)\n",
            "Exception: jieba: file does not exist: /content/dict.txt.big\n"
          ]
        }
      ]
    }
  ]
}